{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fused Layers in Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/\n",
      "Requirement already satisfied: triton-nightly==3.0.0.post20240626041721 in /home/vscode/.cache/pypoetry/virtualenvs/keras-matmulless-b9IALFmu-py3.10/lib/python3.10/site-packages (3.0.0.post20240626041721)\n",
      "Requirement already satisfied: filelock in /home/vscode/.cache/pypoetry/virtualenvs/keras-matmulless-b9IALFmu-py3.10/lib/python3.10/site-packages (from triton-nightly==3.0.0.post20240626041721) (3.15.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/ triton-nightly==3.0.0.post20240626041721"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "os.environ[\"DISABLE_TORCH_COMPILE\"] = \"true\"\n",
    "os.environ[\"TRITON_PRINT_AUTOTUNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the installed triton version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "\n",
    "assert triton.__version__ == \"3.0.0\", f\"Expected Triton to have a version of 3.0.0, but found {triton.__version__}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import other needed stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the autotune config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autotune_config():\n",
    "    return [\n",
    "        triton.Config({}, num_warps=1),\n",
    "        triton.Config({}, num_warps=2),\n",
    "        triton.Config({}, num_warps=4),\n",
    "        triton.Config({}, num_warps=8),\n",
    "        triton.Config({}, num_warps=16),\n",
    "        triton.Config({}, num_warps=32),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Baseline Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the RMSNorm is defined to be\n",
    "$$\n",
    "\\frac{\\mathbf{X}}{\\mathrm{RMS}(\\mathbf{X})}\\odot \\mathbf{G} + \\mathbf{B}\n",
    "$$\n",
    "where $\\mathbf{X}$ is the input tensor, $\\mathbf{G}$ is the 'gain' tensor (gamma in the original LayerNorm paper), and $\\mathbf{B}$ is the bias tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the spec that we will be implementing in Triton. This works for any dimension tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_rms_norm_baseline(x: torch.Tensor, gain, bias, epsilon):\n",
    "    dim = x.shape[-1]\n",
    "    scale = dim**0.5\n",
    "\n",
    "    x_norm = torch.nn.functional.normalize(x, p=2, eps=1e-5, dim=-1) * scale\n",
    "    if gain is not None:\n",
    "        x_norm *= gain\n",
    "    if bias is not None:\n",
    "        x_norm += bias\n",
    "\n",
    "    scale = 127.0 / torch.unsqueeze(torch.max(torch.abs(x_norm), dim=-1).values.clamp_(epsilon), -1)\n",
    "    y = torch.clip(torch.round(x_norm * scale), -128, 127) / scale\n",
    "\n",
    "    return x_norm + (y - x_norm).detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Fused Layer Norm with Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to fuse the RMSNorm and the quantization into one layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: N803\n",
    "@triton.autotune(\n",
    "    configs=get_autotune_config(),\n",
    "    key=[\"N\", \"HAS_GAIN\", \"HAS_BIAS\"],\n",
    ")\n",
    "@triton.jit\n",
    "def quant_rms_norm_2d_fwd_kernel(\n",
    "    # fmt: off\n",
    "    # Pointers to arrays\n",
    "    x_ptr, y_ptr, gain_ptr, bias_ptr, rrms_ptr,\n",
    "    # Strides\n",
    "    stride_x_row,  # How much to increase the pointer when moving by 1 row\n",
    "    stride_y_row,\n",
    "    # Some constants\n",
    "    N,        # Number of columns in X\n",
    "    EPSILON,  # To avoid division by zero\n",
    "    # Meta-parameters\n",
    "    BLOCK_SIZE_N: tl.constexpr,\n",
    "    HAS_GAIN: tl.constexpr,\n",
    "    HAS_BIAS: tl.constexpr\n",
    "    # fmt: on\n",
    "):\n",
    "    \"\"\"\n",
    "    Forward kernel.\n",
    "\n",
    "    Performs RMSNorm on ``X``, followed by 8-bit quantization.\n",
    "    \"\"\"\n",
    "\n",
    "    # Map the PID to the row of X that should be loaded\n",
    "    pid = tl.program_id(0)\n",
    "    x_ptr += pid * stride_x_row\n",
    "\n",
    "    offsets = tl.arange(0, BLOCK_SIZE_N)\n",
    "    mask = offsets < N\n",
    "    x = tl.load(x_ptr + offsets, mask=mask, other=0.0).to(tl.float32)  # Load in higher precision\n",
    "\n",
    "    # Compute reciprocal root mean square (rrms)\n",
    "    mean_of_squares = tl.sum(x * x, axis=0) / N\n",
    "    rrms = 1 / tl.sqrt(mean_of_squares + EPSILON)\n",
    "    tl.store(rrms_ptr + pid, rrms)  # We add PID since that is the row that the RRMS is corresponding to\n",
    "\n",
    "    # Normalize\n",
    "    x_hat = x * rrms\n",
    "\n",
    "    # Apply gain and bias\n",
    "    y = x_hat\n",
    "\n",
    "    if HAS_GAIN:\n",
    "        gain = tl.load(gain_ptr + offsets, mask=mask).to(tl.float32)\n",
    "        y = y * gain\n",
    "    if HAS_BIAS:\n",
    "        bias = tl.load(bias_ptr + offsets, mask=mask).to(tl.float32)\n",
    "        y = y + bias\n",
    "\n",
    "    # Apply 8-bit quantization\n",
    "    scale = 127.0 / tl.maximum(tl.max(tl.abs(y), 0), EPSILON)\n",
    "    y = tl.extra.cuda.libdevice.round(y * scale)  # TODO: This is CUDA only... can we generalize this?\n",
    "    y = tl.maximum(tl.minimum(y, 127), -128) / scale  # The nested max and min creates the clamp/clip function\n",
    "\n",
    "    # Write output\n",
    "    y_ptr += pid * stride_y_row\n",
    "    tl.store(y_ptr + offsets, y, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define the companion function that handles checking and allocation of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: N806, S101\n",
    "def quant_rms_norm_2d_fwd(x: torch.Tensor, gain, bias, epsilon):\n",
    "    \"\"\"\n",
    "    Forward pass.\n",
    "\n",
    "    Performs RMSNorm on ``X``, followed by 8-bit quantization.\n",
    "\n",
    "    Requires CUDA.\n",
    "    \"\"\"\n",
    "\n",
    "    assert x.ndim == 2  # TODO: Support other ndim values?\n",
    "\n",
    "    # Get dimensions\n",
    "    M, N = x.shape\n",
    "\n",
    "    # Validate that the input is OK\n",
    "    assert x.stride(-1) == 1\n",
    "\n",
    "    if gain is not None:\n",
    "        assert gain.shape == (N,)\n",
    "        assert gain.stride(-1) == 1\n",
    "    if bias is not None:\n",
    "        assert bias.shape == (N,)\n",
    "        assert bias.stride(-1) == 1\n",
    "\n",
    "    # Enqueue fused kernel if less than 64KiB per feature\n",
    "    MAX_FUSED_SIZE = 65536 // x.element_size()\n",
    "    BLOCK_SIZE_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n",
    "    if N > BLOCK_SIZE_N:\n",
    "        raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KiB.\")\n",
    "\n",
    "    # Allocate output\n",
    "    y = torch.empty_like(x, dtype=x.dtype)\n",
    "    rrms = torch.empty((M,), dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "    # Run the kernel\n",
    "    with torch.cuda.device(x.device.index):\n",
    "        quant_rms_norm_2d_fwd_kernel[(M,)](\n",
    "            # fmt: off\n",
    "            # Pointers to arrays\n",
    "            x, y, gain, bias, rrms,\n",
    "            # Strides\n",
    "            x.stride(0),\n",
    "            y.stride(0),\n",
    "            # Some constants\n",
    "            N,  # Number of columns in X\n",
    "            epsilon,  # To avoid division by zero\n",
    "            # Meta-parameters\n",
    "            BLOCK_SIZE_N,\n",
    "            gain is not None,\n",
    "            bias is not None\n",
    "            # fmt: on\n",
    "        )\n",
    "\n",
    "    # Return stuff\n",
    "    return y, rrms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The backward pass of the raw RMSNorm layer is a little more involved than the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $\\nabla_\\mathbf{Y}$ to be the gradient of the outputs (i.e., Vector-Jacobian Products (VJP), error signal, or $\\mathrm{d}\\,\\mathrm{out}$), $\\nabla_\\mathbf{X}$ be the required change of $\\mathbf{X}$, and $\\nabla_\\mathbf{G}$ and $\\nabla_\\mathbf{B}$ be defined similarly. Let $\\mathbf{\\hat{X}} = \\frac{\\mathbf{X}}{\\mathrm{RMS}(\\mathbf{X})}$. Then we have\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\nabla_\\mathbf{X} &= \\frac1{\\mathrm{RMS}(\\mathbf{X})} \\left(\\nabla_\\mathbf{Y} \\odot \\mathbf{G} - \\underbrace{\\left(\\frac1N \\mathbf{\\hat{X}} \\cdot \\left(\\nabla_\\mathbf{Y} \\odot \\mathbf{G} \\right) \\right)}_{\\texttt{intermediate\\_const}} \\odot \\mathbf{\\hat{X}}\\right) \\\\\n",
    "    \\nabla_\\mathbf{G} &= \\sum_{i=1}^{N}\\left(\\nabla_\\mathbf{Y} \\odot \\mathbf{\\hat{X}}\\right) & (\\text{sum across rows})\\\\\n",
    "    \\nabla_\\mathbf{B} &= \\sum_{i=1}^{N}\\nabla_\\mathbf{Y} & (\\text{sum across rows})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: N803\n",
    "@triton.autotune(\n",
    "    configs=get_autotune_config(),\n",
    "    key=[\"N\", \"HAS_GAIN\", \"HAS_BIAS\"],\n",
    ")\n",
    "@triton.jit\n",
    "def quant_rms_norm_2d_bwd_kernel(\n",
    "    # fmt: off\n",
    "    # Gradient inputs\n",
    "    grad_output_ptr, dx_ptr, dg_ptr, db_ptr,\n",
    "    # Original inputs\n",
    "    x_ptr, gain_ptr, rrms_ptr,\n",
    "    # Strides\n",
    "    stride_x_row,  # How much to increase the pointer when moving by 1 row\n",
    "    stride_grad_output_row,\n",
    "    stride_dx_row,\n",
    "    # Some constants\n",
    "    M,                 # Number of rows in X\n",
    "    N,                 # Number of columns in X\n",
    "    ROWS_PER_PROGRAM,  # Number of rows of X to compute per program\n",
    "    # Meta-parameters\n",
    "    BLOCK_SIZE_N: tl.constexpr,\n",
    "    HAS_GAIN: tl.constexpr,\n",
    "    HAS_BIAS: tl.constexpr\n",
    "    # fmt: on\n",
    "):\n",
    "    \"\"\"\n",
    "    Backward kernel.\n",
    "\n",
    "    Performs the backward pass of RMSNorm, skipping the quantization step.\n",
    "    \"\"\"\n",
    "\n",
    "    # Map the PID to the elements of `x`, `dx`, `dg`, and `db` that should be computed\n",
    "    pid = tl.program_id(0)\n",
    "    row_start = pid * ROWS_PER_PROGRAM\n",
    "\n",
    "    x_ptr += row_start * stride_x_row\n",
    "    grad_output_ptr += row_start * stride_grad_output_row\n",
    "    dx_ptr += row_start * stride_dx_row\n",
    "\n",
    "    offsets = tl.arange(0, BLOCK_SIZE_N)\n",
    "    mask = offsets < N\n",
    "\n",
    "    # Load gradient array, and prepare gradient and bias output gradient arrays\n",
    "    if HAS_GAIN:\n",
    "        gain = tl.load(gain_ptr + offsets, mask=mask).to(tl.float32)\n",
    "        dg = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n",
    "    if HAS_BIAS:\n",
    "        db = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n",
    "\n",
    "    # Iterate through the rows\n",
    "    row_end = min(row_start + ROWS_PER_PROGRAM, M)\n",
    "    for row in range(row_start, row_end):\n",
    "        # Load data to SRAM\n",
    "        x = tl.load(x_ptr + offsets, mask=mask, other=0.0).to(tl.float32)  # Load in higher precision\n",
    "        grad_output = tl.load(grad_output_ptr + offsets, mask=mask, other=0.0).to(tl.float32)\n",
    "        rrms = tl.load(rrms_ptr + row)  # Load the reciprocal root mean square (rrms)\n",
    "\n",
    "        # Compute `x_hat` and the product of the gradient output with the gain\n",
    "        x_hat = x * rrms\n",
    "\n",
    "        # Apply contributions to the gain and bias gradients\n",
    "        gradient_gain_product = grad_output\n",
    "        if HAS_GAIN:\n",
    "            gradient_gain_product = grad_output * gain\n",
    "            dg += grad_output * x_hat\n",
    "        if HAS_BIAS:\n",
    "            db += grad_output\n",
    "\n",
    "        # Compute `dx`\n",
    "        intermediate_const = tl.sum(x_hat * gradient_gain_product, axis=0) / N\n",
    "        dx = (gradient_gain_product - x_hat * intermediate_const) * rrms\n",
    "\n",
    "        # Write `dx`\n",
    "        tl.store(dx_ptr + offsets, dx, mask=mask)\n",
    "\n",
    "        # Update pointers to move to next row\n",
    "        x_ptr += stride_x_row\n",
    "        grad_output_ptr += stride_grad_output_row\n",
    "        dx_ptr += stride_dx_row\n",
    "\n",
    "    # Once we finished computing all the rows for this program, we can write the final `dg` and `db`\n",
    "    if HAS_GAIN:\n",
    "        tl.store(dg_ptr + pid * N + offsets, dg, mask=mask)\n",
    "    if HAS_BIAS:\n",
    "        tl.store(db_ptr + pid * N + offsets, db, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define the companion function that handles checking and allocation of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: N806, S101\n",
    "import math\n",
    "\n",
    "\n",
    "def quant_rms_norm_2d_bwd(grad_output: torch.Tensor, x: torch.Tensor, gain, bias, rrms):\n",
    "    \"\"\"\n",
    "    Backward  pass.\n",
    "\n",
    "    Performs the backward pass of RMSNorm on ``X``.\n",
    "    \"\"\"\n",
    "\n",
    "    assert x.ndim == 2  # TODO: Support other ndim values?\n",
    "\n",
    "    # Get dimensions\n",
    "    M, N = x.shape\n",
    "\n",
    "    # Validate that the input is OK\n",
    "    assert x.stride(-1) == 1\n",
    "    assert grad_output.shape == (M, N)\n",
    "    assert grad_output.stride(-1) == 1\n",
    "\n",
    "    if gain is not None:\n",
    "        assert gain.shape == (N,)\n",
    "        assert gain.stride(-1) == 1\n",
    "    if bias is not None:\n",
    "        assert bias.shape == (N,)\n",
    "        assert bias.stride(-1) == 1\n",
    "\n",
    "    # Enqueue fused kernel if less than 64KiB per feature\n",
    "    MAX_FUSED_SIZE = 65536 // x.element_size()\n",
    "    BLOCK_SIZE_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n",
    "    if N > BLOCK_SIZE_N:\n",
    "        raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KiB.\")\n",
    "\n",
    "    # Allocate output\n",
    "    multi_processor_count = torch.cuda.get_device_properties(x.device).multi_processor_count\n",
    "\n",
    "    dx = torch.empty_like(x, dtype=x.dtype)\n",
    "    if gain is not None:\n",
    "        # This is temporary as we still need to sum across the rows later\n",
    "        dg_temp = torch.empty((multi_processor_count, N), dtype=torch.float32, device=gain.device)\n",
    "    else:\n",
    "        dg_temp = None\n",
    "    if bias is not None:\n",
    "        db_temp = torch.empty((multi_processor_count, N), dtype=torch.float32, device=bias.device)\n",
    "    else:\n",
    "        db_temp = None\n",
    "\n",
    "    # Run the kernel\n",
    "    # TODO: We could make this faster by using a technique like shown in\n",
    "    #   https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html#backward-pass\n",
    "    rows_per_program = math.ceil(M / multi_processor_count)\n",
    "    with torch.cuda.device(x.device.index):\n",
    "        quant_rms_norm_2d_bwd_kernel[(multi_processor_count,)](\n",
    "            # fmt: off\n",
    "            # Gradient inputs\n",
    "            grad_output, dx, dg_temp, db_temp,\n",
    "            # Original inputs\n",
    "            x, gain, rrms,\n",
    "            # Strides\n",
    "            x.stride(0),\n",
    "            grad_output.stride(0),\n",
    "            dx.stride(0),\n",
    "            # Some constants\n",
    "            M,\n",
    "            N,\n",
    "            rows_per_program,\n",
    "            # Meta-parameters\n",
    "            BLOCK_SIZE_N,\n",
    "            gain is not None,\n",
    "            bias is not None\n",
    "            # fmt: on\n",
    "        )\n",
    "\n",
    "    # Fix the summing of `dg` and `db`\n",
    "    if gain is not None:\n",
    "        dg = dg_temp.sum(0).to(gain.dtype)\n",
    "    else:\n",
    "        dg = None\n",
    "\n",
    "    if bias is not None:\n",
    "        db = db_temp.sum(0).to(bias.dtype)\n",
    "    else:\n",
    "        db = None\n",
    "\n",
    "    # Return stuff\n",
    "    return dx, dg, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Making the Autograd Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now have both the forward and backward pass, we can make a `torch.autograd.Function` that comprises both the forward and backward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantRMSNorm2DFn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, gain, bias, epsilon=1e-5):\n",
    "        # Run the forward function\n",
    "        y, rrms = quant_rms_norm_2d_fwd(x, gain, bias, epsilon)\n",
    "\n",
    "        # Save tensors for backward pass later\n",
    "        ctx.save_for_backward(x, gain, bias, rrms)\n",
    "\n",
    "        # Return the result of the forward pass\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Retrieve stored tensors\n",
    "        x, gain, bias, rrms = ctx.saved_tensors\n",
    "\n",
    "        # Perform backward pass\n",
    "        dx, dg, db = quant_rms_norm_2d_bwd(grad_output, x, gain, bias, rrms)\n",
    "\n",
    "        # Return the gradients\n",
    "        return dx, dg, db, None  # No gradient for `epsilon`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now wrap this in a standard function-like format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_rms_norm_2d_triton(x, gain, bias, epsilon=1e-5):\n",
    "    return QuantRMSNorm2DFn.apply(x, gain, bias, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8cd8981f90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(8192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define the tensors used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_SHAPE = (10, 10)\n",
    "WEIGHT_SHAPE = (X_SHAPE[-1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(X_SHAPE, device=\"cuda\", requires_grad=True)\n",
    "gain = torch.rand(WEIGHT_SHAPE, device=\"cuda\", requires_grad=True)\n",
    "bias = torch.rand(WEIGHT_SHAPE, device=\"cuda\", requires_grad=True)\n",
    "dy = 0.1 * torch.randn_like(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the baseline result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5447, 1.7738, 0.3631, 1.1034, 0.7402, 0.8799, 0.3771, 0.8939, 0.5587,\n",
       "         0.6425],\n",
       "        [0.4221, 0.2985, 0.3500, 1.3074, 0.7000, 1.2354, 0.1956, 0.6280, 1.2868,\n",
       "         0.3191],\n",
       "        [0.6747, 1.4522, 0.3545, 1.2235, 0.7318, 0.7776, 0.2630, 1.1892, 0.9834,\n",
       "         0.5717],\n",
       "        [0.4624, 1.3345, 0.3468, 1.2189, 0.6725, 1.0193, 0.7251, 1.0298, 0.8511,\n",
       "         0.7145],\n",
       "        [0.4530, 0.3867, 0.3536, 1.1049, 0.6961, 1.1049, 0.5303, 1.4032, 0.7623,\n",
       "         1.0938],\n",
       "        [0.4230, 0.6294, 0.3508, 1.3104, 0.6913, 1.1350, 0.4850, 1.0009, 0.8151,\n",
       "         0.9493],\n",
       "        [0.9127, 1.2201, 0.3555, 1.0664, 0.6245, 1.0856, 0.3170, 0.8838, 1.1913,\n",
       "         0.9319],\n",
       "        [1.2123, 0.8914, 0.3447, 1.1410, 0.6774, 0.9627, 0.2020, 1.5094, 1.0578,\n",
       "         0.3922],\n",
       "        [1.0413, 1.0617, 0.3369, 1.1536, 0.7350, 0.9188, 0.1327, 1.2965, 0.8269,\n",
       "         1.0005],\n",
       "        [0.8077, 1.3206, 0.3462, 1.1924, 0.6411, 1.1539, 0.2821, 1.6283, 1.0129,\n",
       "         0.1667]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ref = quant_rms_norm_baseline(x, gain, bias, 1e-5)\n",
    "y_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare that with the Triton result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton autotuning for function quant_rms_norm_2d_fwd_kernel finished after 1.26s; best config selected: num_warps: 8, num_ctas: 1, num_stages: 2, maxnreg: None;\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5447, 1.7737, 0.3631, 1.1034, 0.7402, 0.8799, 0.3771, 0.8939, 0.5587,\n",
       "         0.6425],\n",
       "        [0.4221, 0.2985, 0.3500, 1.3074, 0.7000, 1.2354, 0.1956, 0.6280, 1.2868,\n",
       "         0.3191],\n",
       "        [0.6747, 1.4522, 0.3545, 1.2235, 0.7318, 0.7776, 0.2630, 1.1892, 0.9834,\n",
       "         0.5717],\n",
       "        [0.4623, 1.3345, 0.3468, 1.2189, 0.6725, 1.0193, 0.7250, 1.0298, 0.8511,\n",
       "         0.7145],\n",
       "        [0.4530, 0.3867, 0.3535, 1.1048, 0.6961, 1.1048, 0.5303, 1.4032, 0.7623,\n",
       "         1.0938],\n",
       "        [0.4230, 0.6294, 0.3508, 1.3104, 0.6913, 1.1350, 0.4850, 1.0009, 0.8151,\n",
       "         0.9493],\n",
       "        [0.9126, 1.2201, 0.3555, 1.0664, 0.6244, 1.0856, 0.3170, 0.8838, 1.1912,\n",
       "         0.9319],\n",
       "        [1.2123, 0.8914, 0.3447, 1.1409, 0.6774, 0.9627, 0.2020, 1.5094, 1.0578,\n",
       "         0.3922],\n",
       "        [1.0413, 1.0617, 0.3369, 1.1536, 0.7350, 0.9188, 0.1327, 1.2965, 0.8269,\n",
       "         1.0004],\n",
       "        [0.8077, 1.3206, 0.3462, 1.1923, 0.6410, 1.1539, 0.2821, 1.6283, 1.0129,\n",
       "         0.1667]], device='cuda:0', grad_fn=<QuantRMSNorm2DFnBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tri = quant_rms_norm_2d_triton(x, gain, bias, 1e-5)\n",
    "y_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Triton and Baseline match\n"
     ]
    }
   ],
   "source": [
    "if torch.allclose(y_tri, y_ref, atol=1e-3):\n",
    "    print(\"✅ Triton and Baseline match\")\n",
    "else:\n",
    "    raise ValueError(\"❌ Triton and Baseline differ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the baseline result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0078,  0.0402, -0.0129, -0.0158, -0.0524, -0.0079, -0.0948,  0.1027,\n",
      "          0.1327, -0.0244],\n",
      "        [ 0.0709,  0.0114,  0.0033, -0.0562,  0.0288, -0.0261,  0.0541,  0.0599,\n",
      "          0.0078,  0.1711],\n",
      "        [-0.0591, -0.0919,  0.0141,  0.0313,  0.0295,  0.0317,  0.1106,  0.0467,\n",
      "          0.0569, -0.0939],\n",
      "        [ 0.1088, -0.0116,  0.0183,  0.0447,  0.0171,  0.0080, -0.0127, -0.2264,\n",
      "         -0.0414,  0.1106],\n",
      "        [-0.0454,  0.0199,  0.0118,  0.0671,  0.0293, -0.0350, -0.0588, -0.1081,\n",
      "          0.0319,  0.0749],\n",
      "        [ 0.0250,  0.2641, -0.0211, -0.0775, -0.0381, -0.0242, -0.0157,  0.1250,\n",
      "          0.1453, -0.0887],\n",
      "        [-0.1169,  0.0709,  0.0245,  0.0471,  0.0141,  0.0007,  0.0813,  0.2232,\n",
      "          0.0945, -0.2183],\n",
      "        [-0.0392,  0.2809, -0.0388, -0.0592, -0.0410, -0.1031,  0.0837, -0.0956,\n",
      "          0.0940,  0.0461],\n",
      "        [-0.0417,  0.1409,  0.0034, -0.0495, -0.0641,  0.0404, -0.0203, -0.0187,\n",
      "         -0.2352,  0.1868],\n",
      "        [ 0.1165, -0.0732,  0.0315,  0.0366,  0.0242, -0.0287,  0.0871, -0.1121,\n",
      "          0.0582,  0.0067]], device='cuda:0')\n",
      "tensor([-0.0424,  0.1841,  0.0366, -0.3423, -0.3268, -0.4209, -0.0460, -0.2440,\n",
      "         0.2947,  0.1207], device='cuda:0')\n",
      "tensor([ 0.0756,  0.3781,  0.1735, -0.0483,  0.0105, -0.2859,  0.2894, -0.0417,\n",
      "         0.2777,  0.1915], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "y_ref.backward(dy, retain_graph=True)\n",
    "dx_ref, dg_ref, db_ref = [tensor.grad.clone() for tensor in [x, gain, bias]]\n",
    "x.grad, gain.grad, bias.grad = None, None, None  # Reset gradients for use later\n",
    "\n",
    "print(dx_ref)\n",
    "print(dg_ref)\n",
    "print(db_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with the Triton result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Triton autotuning for function quant_rms_norm_2d_bwd_kernel finished after 0.73s; best config selected: num_warps: 1, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "tensor([[-0.0078,  0.0402, -0.0129, -0.0158, -0.0524, -0.0079, -0.0948,  0.1027,\n",
      "          0.1327, -0.0244],\n",
      "        [ 0.0709,  0.0114,  0.0033, -0.0562,  0.0288, -0.0261,  0.0541,  0.0599,\n",
      "          0.0078,  0.1711],\n",
      "        [-0.0591, -0.0919,  0.0141,  0.0313,  0.0295,  0.0317,  0.1106,  0.0467,\n",
      "          0.0569, -0.0939],\n",
      "        [ 0.1088, -0.0116,  0.0183,  0.0447,  0.0171,  0.0080, -0.0127, -0.2264,\n",
      "         -0.0414,  0.1106],\n",
      "        [-0.0454,  0.0199,  0.0118,  0.0671,  0.0293, -0.0350, -0.0588, -0.1081,\n",
      "          0.0319,  0.0749],\n",
      "        [ 0.0250,  0.2641, -0.0211, -0.0775, -0.0380, -0.0242, -0.0157,  0.1250,\n",
      "          0.1453, -0.0887],\n",
      "        [-0.1169,  0.0709,  0.0245,  0.0471,  0.0141,  0.0007,  0.0813,  0.2232,\n",
      "          0.0945, -0.2183],\n",
      "        [-0.0392,  0.2809, -0.0388, -0.0592, -0.0410, -0.1031,  0.0837, -0.0956,\n",
      "          0.0940,  0.0461],\n",
      "        [-0.0417,  0.1409,  0.0034, -0.0495, -0.0641,  0.0404, -0.0203, -0.0186,\n",
      "         -0.2352,  0.1868],\n",
      "        [ 0.1164, -0.0732,  0.0315,  0.0366,  0.0242, -0.0287,  0.0871, -0.1121,\n",
      "          0.0582,  0.0067]], device='cuda:0')\n",
      "tensor([-0.0424,  0.1841,  0.0366, -0.3423, -0.3268, -0.4209, -0.0460, -0.2440,\n",
      "         0.2947,  0.1207], device='cuda:0')\n",
      "tensor([ 0.0756,  0.3781,  0.1735, -0.0483,  0.0105, -0.2859,  0.2894, -0.0417,\n",
      "         0.2777,  0.1915], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "y_tri.backward(dy, retain_graph=True)\n",
    "dx_tri, dg_tri, db_tri = [tensor.grad.clone() for tensor in [x, gain, bias]]\n",
    "\n",
    "print(dx_tri)\n",
    "print(dg_tri)\n",
    "print(db_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Triton and Baseline match\n"
     ]
    }
   ],
   "source": [
    "if (\n",
    "    torch.allclose(dx_tri, dx_ref, atol=1e-3)\n",
    "    and torch.allclose(dg_tri, dg_ref, atol=1e-3)\n",
    "    and torch.allclose(db_tri, db_ref, atol=1e-3)\n",
    "):\n",
    "    print(\"✅ Triton and Baseline match\")\n",
    "else:\n",
    "    raise ValueError(\"❌ Triton and Baseline differ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform benchmarking for both the forward and backward passes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define the benchmark configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=[\"M\", \"N\"],  # Argument names to use as an x-axis for the plot\n",
    "        x_vals=[256 * i for i in range(1, 17)],  # Different possible values for `x_name`\n",
    "        line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "        # Possible values for `line_arg`\n",
    "        line_vals=[\"baseline\", \"triton\"],  # Label name for the lines\n",
    "        line_names=[\"Baseline\", \"Triton\"],  # Line styles\n",
    "        styles=[(\"green\", \"-\"), (\"blue\", \"-\")],\n",
    "        ylabel=\"GB/s\",  # Label name for the y-axis\n",
    "        plot_name=f\"quant-rms-norm-{mode}\",  # Name for the plot, used also as a file name for saving the plot.\n",
    "        args={\"mode\": mode},\n",
    "    )\n",
    "    for mode in [\"forward\", \"backward\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the actual benchmark tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: E731\n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(M, N, provider, mode):\n",
    "    print(f\"Trial when M = {M} and N = {N} for {provider} for {mode} pass\")\n",
    "    x_shape = (M, N)\n",
    "    weight_shape = (N,)\n",
    "\n",
    "    x = torch.rand(x_shape, device=\"cuda\", requires_grad=True)\n",
    "    gain = torch.rand(weight_shape, device=\"cuda\", requires_grad=True)\n",
    "    bias = torch.rand(weight_shape, device=\"cuda\", requires_grad=True)\n",
    "    dy = 0.1 * torch.randn_like(x)\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "\n",
    "    def y_fwd():\n",
    "        if provider == \"baseline\":\n",
    "            return quant_rms_norm_baseline(x, gain, bias, 1e-5)\n",
    "        if provider == \"triton\":\n",
    "            return quant_rms_norm_2d_triton(x, gain, bias, 1e-5)\n",
    "\n",
    "    if mode == \"forward\":\n",
    "        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(y_fwd, quantiles=quantiles)\n",
    "    else:  # Backward\n",
    "        y = y_fwd()\n",
    "        gbps = lambda ms: 3 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: y.backward(dy, retain_graph=True), quantiles=quantiles, grad_to_none=[x]\n",
    "        )\n",
    "\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "\n",
    "# benchmark.run(print_data=True)  # TODO: Re-enable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nD Fused Layer Norm with Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive (Both Forward and Backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive method for implementing the nD case is to iterate through the batch dimensions, but *not using Triton to do so*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_rms_norm_triton_naive(x: torch.Tensor, gain, bias, epsilon=1e-5):\n",
    "    x_shape = x.shape\n",
    "\n",
    "    num_stacked_matrices = 1\n",
    "    for i in range(x.ndim - 2):  # The last 2 indices are the matrices\n",
    "        num_stacked_matrices *= x_shape[i]\n",
    "\n",
    "    # Identify the shape of the matrices and vectors that will actually be multiplied\n",
    "    matrix_shape = (x_shape[-2], x_shape[-1])\n",
    "    matrix_stride = x_shape[-2] * x_shape[-1]\n",
    "\n",
    "    # Flatten the input arrays for easier processing\n",
    "    x_flat = x.flatten()\n",
    "\n",
    "    # Determine output size\n",
    "    output_size = 1\n",
    "    for dim in x_shape:\n",
    "        output_size *= dim\n",
    "    output_flat = torch.zeros((output_size,), dtype=x.dtype, device=x.device)\n",
    "\n",
    "    for i in range(num_stacked_matrices):\n",
    "        x_part = x_flat[i * matrix_stride : (i + 1) * matrix_stride].reshape(matrix_shape)\n",
    "        x_quantized = quant_rms_norm_2d_triton(x_part, gain, bias, epsilon=epsilon)\n",
    "        output_flat[i * matrix_stride : (i + 1) * matrix_stride] = x_quantized.flatten()\n",
    "\n",
    "    return output_flat.reshape(x_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this is a ***painfully*** slow process, especially for the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define the Triton kernel for the nD case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: N803\n",
    "@triton.autotune(\n",
    "    configs=get_autotune_config(),\n",
    "    key=[\"N\", \"HAS_GAIN\", \"HAS_BIAS\"],\n",
    ")\n",
    "@triton.jit\n",
    "def quant_rms_norm_nd_fwd_kernel(\n",
    "    # fmt: off\n",
    "    # Pointers to arrays\n",
    "    x_ptr, y_ptr, gain_ptr, bias_ptr, rrms_ptr,\n",
    "    # Strides\n",
    "    stride_x_matrix,  # How much to increase the pointer when moving by 1 matrix\n",
    "    stride_x_row,     # How much to increase the pointer when moving by 1 row\n",
    "    stride_y_row,\n",
    "    # Some constants\n",
    "    M,        # Number of rows in X\n",
    "    N,        # Number of columns in X\n",
    "    EPSILON,  # To avoid division by zero\n",
    "    # Meta-parameters\n",
    "    BLOCK_SIZE_N: tl.constexpr,\n",
    "    HAS_GAIN: tl.constexpr,\n",
    "    HAS_BIAS: tl.constexpr\n",
    "    # fmt: on\n",
    "):\n",
    "    \"\"\"\n",
    "    Forward kernel (nD case).\n",
    "\n",
    "    Performs RMSNorm on ``X``, followed by 8-bit quantization.\n",
    "    \"\"\"\n",
    "\n",
    "    # Map the PID to the appropriate matrix and row that should be loaded\n",
    "    pid_matrix = tl.program_id(0)  # Tells us which matrix to look at\n",
    "    x_ptr += pid_matrix * stride_x_matrix\n",
    "\n",
    "    pid_row = tl.program_id(1)  # Tells us which row of the matrix to look at\n",
    "    x_ptr += pid_row * stride_x_row\n",
    "\n",
    "    offsets = tl.arange(0, BLOCK_SIZE_N)\n",
    "    mask = offsets < N\n",
    "    x = tl.load(x_ptr + offsets, mask=mask, other=0.0).to(tl.float32)  # Load in higher precision\n",
    "\n",
    "    # Compute reciprocal root mean square (rrms)\n",
    "    mean_of_squares = tl.sum(x * x, axis=0) / N\n",
    "    rrms = 1 / tl.sqrt(mean_of_squares + EPSILON)\n",
    "\n",
    "    tl.store(rrms_ptr + pid_matrix * M + pid_row, rrms)\n",
    "\n",
    "    # Normalize\n",
    "    x_hat = x * rrms\n",
    "\n",
    "    # Apply gain and bias\n",
    "    y = x_hat\n",
    "\n",
    "    if HAS_GAIN:\n",
    "        gain = tl.load(gain_ptr + offsets, mask=mask).to(tl.float32)\n",
    "        y = y * gain\n",
    "    if HAS_BIAS:\n",
    "        bias = tl.load(bias_ptr + offsets, mask=mask).to(tl.float32)\n",
    "        y = y + bias\n",
    "\n",
    "    # Apply 8-bit quantization\n",
    "    scale = 127.0 / tl.maximum(tl.max(tl.abs(y), 0), EPSILON)\n",
    "    y = tl.extra.cuda.libdevice.round(y * scale)  # TODO: This is CUDA only... can we generalize this?\n",
    "    y = tl.maximum(tl.minimum(y, 127), -128) / scale  # The nested max and min creates the clamp/clip function\n",
    "\n",
    "    # Write output\n",
    "    y_ptr += pid_matrix * stride_x_matrix\n",
    "    y_ptr += pid_row * stride_y_row\n",
    "    tl.store(y_ptr + offsets, y, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define the companion function that handles checking and allocation of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: N806, S101\n",
    "def quant_rms_norm_nd_fwd(x: torch.Tensor, gain, bias, num_stacked_matrices, matrix_shape, epsilon):\n",
    "    \"\"\"\n",
    "    Forward pass (nD case).\n",
    "\n",
    "    Performs RMSNorm on ``X``, followed by 8-bit quantization.\n",
    "\n",
    "    Requires CUDA.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get dimensions\n",
    "    M, N = matrix_shape\n",
    "\n",
    "    # Validate that the input is OK\n",
    "    assert x.stride(-1) == 1\n",
    "\n",
    "    if gain is not None:\n",
    "        assert gain.shape == (N,)\n",
    "        assert gain.stride(-1) == 1\n",
    "    if bias is not None:\n",
    "        assert bias.shape == (N,)\n",
    "        assert bias.stride(-1) == 1\n",
    "\n",
    "    # Enqueue fused kernel if less than 64KiB per feature\n",
    "    MAX_FUSED_SIZE = 65536 // x.element_size()\n",
    "    BLOCK_SIZE_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n",
    "    if N > BLOCK_SIZE_N:\n",
    "        raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KiB.\")\n",
    "\n",
    "    # Allocate output\n",
    "    y = torch.empty_like(x, dtype=x.dtype)\n",
    "    rrms = torch.empty(x.shape[:-1], dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "    # Run the kernel\n",
    "    with torch.cuda.device(x.device.index):\n",
    "        quant_rms_norm_nd_fwd_kernel[(num_stacked_matrices, M)](\n",
    "            # fmt: off\n",
    "            # Pointers to arrays\n",
    "            x, y, gain, bias, rrms,\n",
    "            # Strides\n",
    "            x.stride(-3),\n",
    "            x.stride(-2),\n",
    "            y.stride(-2),\n",
    "            # Some constants\n",
    "            M,  # Number of rows in X\n",
    "            N,  # Number of columns in X\n",
    "            epsilon,  # To avoid division by zero\n",
    "            # Meta-parameters\n",
    "            BLOCK_SIZE_N,\n",
    "            gain is not None,\n",
    "            bias is not None\n",
    "            # fmt: on\n",
    "        )\n",
    "\n",
    "    # Return stuff\n",
    "    return y, rrms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we first define the kernel for the nD case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: N803\n",
    "@triton.autotune(\n",
    "    configs=get_autotune_config(),\n",
    "    key=[\"N\", \"HAS_GAIN\", \"HAS_BIAS\"],\n",
    ")\n",
    "@triton.jit\n",
    "def quant_rms_norm_nd_bwd_kernel(\n",
    "    # fmt: off\n",
    "    # Gradient inputs\n",
    "    grad_output_ptr, dx_ptr, dg_ptr, db_ptr,\n",
    "    # Original inputs\n",
    "    x_ptr, gain_ptr, rrms_ptr,\n",
    "    # Strides\n",
    "    stride_matrix,  # How much to increase the X pointer when moving by 1 matrix\n",
    "    stride_x_row,     # How much to increase the X pointer when moving by 1 row\n",
    "    stride_grad_output_row,\n",
    "    stride_dx_row,\n",
    "    # Some constants\n",
    "    M,                 # Number of rows in X\n",
    "    N,                 # Number of columns in X\n",
    "    ROWS_PER_PROGRAM,  # Number of rows of X to compute per program\n",
    "    MULTI_PROCESSOR_COUNT,\n",
    "    # Meta-parameters\n",
    "    BLOCK_SIZE_N: tl.constexpr,\n",
    "    HAS_GAIN: tl.constexpr,\n",
    "    HAS_BIAS: tl.constexpr\n",
    "    # fmt: on\n",
    "):\n",
    "    \"\"\"\n",
    "    Backward kernel (nD case).\n",
    "\n",
    "    Performs the backward pass of RMSNorm, skipping the quantization step.\n",
    "    \"\"\"\n",
    "\n",
    "    # Map the PID to the elements of `x`, `dx`, `dg`, and `db` that should be computed\n",
    "    pid_matrix = tl.program_id(0)\n",
    "    pid_row = tl.program_id(1)\n",
    "\n",
    "    row_start = pid_row * ROWS_PER_PROGRAM\n",
    "\n",
    "    x_ptr += pid_matrix * stride_matrix + row_start * stride_x_row\n",
    "    grad_output_ptr += pid_matrix * stride_matrix + row_start * stride_grad_output_row\n",
    "    dx_ptr += pid_matrix * stride_matrix + row_start * stride_dx_row\n",
    "\n",
    "    offsets = tl.arange(0, BLOCK_SIZE_N)\n",
    "    mask = offsets < N\n",
    "\n",
    "    # Load gradient array, and prepare gradient and bias output gradient arrays\n",
    "    if HAS_GAIN:\n",
    "        gain = tl.load(gain_ptr + offsets, mask=mask).to(tl.float32)\n",
    "        dg = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n",
    "    if HAS_BIAS:\n",
    "        db = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n",
    "\n",
    "    # Iterate through the rows\n",
    "    row_end = min(row_start + ROWS_PER_PROGRAM, M)\n",
    "    for row in range(row_start, row_end):\n",
    "        # Load data to SRAM\n",
    "        x = tl.load(x_ptr + offsets, mask=mask, other=0.0).to(tl.float32)  # Load in higher precision\n",
    "        grad_output = tl.load(grad_output_ptr + offsets, mask=mask, other=0.0).to(tl.float32)\n",
    "        rrms = tl.load(rrms_ptr + pid_matrix * M + row)  # Load the reciprocal root mean square (rrms)\n",
    "\n",
    "        # Compute `x_hat` and the product of the gradient output with the gain\n",
    "        x_hat = x * rrms\n",
    "\n",
    "        # Apply contributions to the gain and bias gradients\n",
    "        gradient_gain_product = grad_output\n",
    "        if HAS_GAIN:\n",
    "            gradient_gain_product = grad_output * gain\n",
    "            dg += grad_output * x_hat\n",
    "        if HAS_BIAS:\n",
    "            db += grad_output\n",
    "\n",
    "        # Compute `dx`\n",
    "        intermediate_const = tl.sum(x_hat * gradient_gain_product, axis=0) / N\n",
    "        dx = (gradient_gain_product - x_hat * intermediate_const) * rrms\n",
    "\n",
    "        # Write `dx`\n",
    "        tl.store(dx_ptr + offsets, dx, mask=mask)\n",
    "\n",
    "        # Update pointers to move to next row\n",
    "        x_ptr += stride_x_row\n",
    "        grad_output_ptr += stride_grad_output_row\n",
    "        dx_ptr += stride_dx_row\n",
    "\n",
    "    # Once we finished computing all the rows for this program, we can write the final `dg` and `db`\n",
    "    if HAS_GAIN:\n",
    "        tl.store(dg_ptr + pid_matrix * MULTI_PROCESSOR_COUNT * N + pid_row * N + offsets, dg, mask=mask)\n",
    "    if HAS_BIAS:\n",
    "        tl.store(db_ptr + pid_matrix * MULTI_PROCESSOR_COUNT * N + pid_row * N + offsets, db, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define the companion function that handles checking and allocation of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: N806, S101\n",
    "import math\n",
    "\n",
    "\n",
    "def quant_rms_norm_nd_bwd(\n",
    "    grad_output: torch.Tensor, x: torch.Tensor, gain, bias, rrms, num_stacked_matrices, matrix_shape\n",
    "):\n",
    "    \"\"\"\n",
    "    Backward pass (nD case).\n",
    "\n",
    "    Performs the backward pass of RMSNorm on ``X``.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get dimensions\n",
    "    M, N = matrix_shape\n",
    "\n",
    "    # Validate that the input is OK\n",
    "    assert x.stride(-1) == 1\n",
    "    assert grad_output.shape == x.shape\n",
    "    assert grad_output.stride(-1) == 1\n",
    "\n",
    "    if gain is not None:\n",
    "        assert gain.shape == (N,)\n",
    "        assert gain.stride(-1) == 1\n",
    "    if bias is not None:\n",
    "        assert bias.shape == (N,)\n",
    "        assert bias.stride(-1) == 1\n",
    "\n",
    "    # Enqueue fused kernel if less than 64KiB per feature\n",
    "    MAX_FUSED_SIZE = 65536 // x.element_size()\n",
    "    BLOCK_SIZE_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n",
    "    if N > BLOCK_SIZE_N:\n",
    "        raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KiB.\")\n",
    "\n",
    "    # Allocate output\n",
    "    multi_processor_count = torch.cuda.get_device_properties(x.device).multi_processor_count\n",
    "\n",
    "    dx = torch.empty_like(x, dtype=x.dtype)\n",
    "    if gain is not None:\n",
    "        # This is temporary as we still need to sum across the rows later\n",
    "        dg_temp = torch.empty((num_stacked_matrices, multi_processor_count, N), dtype=torch.float32, device=gain.device)\n",
    "    else:\n",
    "        dg_temp = None\n",
    "    if bias is not None:\n",
    "        db_temp = torch.empty((num_stacked_matrices, multi_processor_count, N), dtype=torch.float32, device=bias.device)\n",
    "    else:\n",
    "        db_temp = None\n",
    "\n",
    "    # Run the kernel\n",
    "    # TODO: We could make this faster by using a technique like shown in\n",
    "    #   https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html#backward-pass\n",
    "    rows_per_program = math.ceil(M / multi_processor_count)\n",
    "    with torch.cuda.device(x.device.index):\n",
    "        quant_rms_norm_nd_bwd_kernel[(num_stacked_matrices, multi_processor_count)](\n",
    "            # fmt: off\n",
    "            # Gradient inputs\n",
    "            grad_output, dx, dg_temp, db_temp,\n",
    "            # Original inputs\n",
    "            x, gain, rrms,\n",
    "            # Strides\n",
    "            x.stride(-3),\n",
    "            x.stride(-2),\n",
    "            grad_output.stride(-2),\n",
    "            dx.stride(-2),\n",
    "            # Some constants\n",
    "            M,\n",
    "            N,\n",
    "            rows_per_program,\n",
    "            multi_processor_count,\n",
    "            # Meta-parameters\n",
    "            BLOCK_SIZE_N,\n",
    "            gain is not None,\n",
    "            bias is not None\n",
    "            # fmt: on\n",
    "        )\n",
    "\n",
    "    # Fix the summing of `dg` and `db`\n",
    "    if gain is not None:\n",
    "        dg = dg_temp.sum((0, 1)).to(gain.dtype)\n",
    "    else:\n",
    "        dg = None\n",
    "\n",
    "    if bias is not None:\n",
    "        db = db_temp.sum((0, 1)).to(bias.dtype)\n",
    "    else:\n",
    "        db = None\n",
    "\n",
    "    # Return stuff\n",
    "    return dx, dg, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the Autograd Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we now have both the forward and backward pass, we can make a `torch.autograd.Function` that comprises both the forward and backward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantRMSNormNDFn(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, gain, bias, epsilon=1e-5):\n",
    "        x_shape = x.shape\n",
    "\n",
    "        num_stacked_matrices = 1\n",
    "        for i in range(x.ndim - 2):  # The last 2 indices are the matrices\n",
    "            num_stacked_matrices *= x_shape[i]\n",
    "\n",
    "        # Identify the shape of the matrices and vectors that will actually be multiplied\n",
    "        matrix_shape = (x_shape[-2], x_shape[-1])\n",
    "\n",
    "        # Run the forward function\n",
    "        y, rrms = quant_rms_norm_nd_fwd(x, gain, bias, num_stacked_matrices, matrix_shape, epsilon)\n",
    "\n",
    "        # Save tensors for backward pass later\n",
    "        ctx.save_for_backward(x, gain, bias, rrms)\n",
    "        ctx.num_stacked_matrices = num_stacked_matrices\n",
    "        ctx.matrix_shape = matrix_shape\n",
    "\n",
    "        # Return the result of the forward pass\n",
    "        return y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Retrieve stored values\n",
    "        x, gain, bias, rrms = ctx.saved_tensors\n",
    "        num_stacked_matrices = ctx.num_stacked_matrices\n",
    "        matrix_shape = ctx.matrix_shape\n",
    "\n",
    "        # Perform backward pass\n",
    "        dx, dg, db = quant_rms_norm_nd_bwd(grad_output, x, gain, bias, rrms, num_stacked_matrices, matrix_shape)\n",
    "\n",
    "        # Return the gradients\n",
    "        return dx, dg, db, None  # No gradient for `epsilon`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now wrap this in a standard function-like format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_rms_norm_triton(x, gain, bias, epsilon=1e-5):\n",
    "    return QuantRMSNormNDFn.apply(x, gain, bias, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset the seed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8cd8981f90>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(8192)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the arrays used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_SHAPE = (2, 2, 3, 3)\n",
    "WEIGHT_SHAPE = (X_SHAPE[-1],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(X_SHAPE, device=\"cuda\", requires_grad=True)\n",
    "gain = torch.rand(WEIGHT_SHAPE, device=\"cuda\", requires_grad=True)\n",
    "bias = torch.rand(WEIGHT_SHAPE, device=\"cuda\", requires_grad=True)\n",
    "dy = 0.1 * torch.randn_like(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the baseline result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.4849, 1.3387, 0.3479],\n",
       "          [0.6399, 1.5333, 0.3381],\n",
       "          [0.8744, 1.0782, 0.3481]],\n",
       "\n",
       "         [[1.1459, 0.5684, 0.3338],\n",
       "          [0.6621, 1.4014, 0.3421],\n",
       "          [1.1625, 0.3845, 0.3387]]],\n",
       "\n",
       "\n",
       "        [[[1.0681, 0.4710, 0.3448],\n",
       "          [0.8912, 0.9202, 0.3478],\n",
       "          [1.1505, 0.3533, 0.3352]],\n",
       "\n",
       "         [[0.7796, 1.2859, 0.3443],\n",
       "          [0.4589, 1.3246, 0.3546],\n",
       "          [0.9526, 0.8401, 0.3450]]]], device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_ref = quant_rms_norm_baseline(x, gain, bias, 1e-5)\n",
    "y_ref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare that with the Triton result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.4849, 1.3387, 0.3479],\n",
       "          [0.6399, 1.5333, 0.3381],\n",
       "          [0.8744, 1.0781, 0.3481]],\n",
       "\n",
       "         [[1.1458, 0.5684, 0.3338],\n",
       "          [0.6621, 1.4014, 0.3421],\n",
       "          [1.1625, 0.3845, 0.3387]]],\n",
       "\n",
       "\n",
       "        [[[1.0681, 0.4710, 0.3448],\n",
       "          [0.8912, 0.9202, 0.3478],\n",
       "          [1.1504, 0.3533, 0.3352]],\n",
       "\n",
       "         [[0.7796, 1.2859, 0.3443],\n",
       "          [0.4589, 1.3246, 0.3546],\n",
       "          [0.9526, 0.8401, 0.3450]]]], device='cuda:0',\n",
       "       grad_fn=<QuantRMSNormNDFnBackward>)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tri = quant_rms_norm_triton(x, gain, bias, 1e-5)\n",
    "y_tri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Triton and Baseline match\n"
     ]
    }
   ],
   "source": [
    "if torch.allclose(y_tri, y_ref, atol=1e-3):\n",
    "    print(\"✅ Triton and Baseline match\")\n",
    "else:\n",
    "    raise ValueError(\"❌ Triton and Baseline differ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the baseline result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0085,  0.0178, -0.0186],\n",
      "          [ 0.1165, -0.0835,  0.1077],\n",
      "          [-0.1970,  0.2153,  0.0060]],\n",
      "\n",
      "         [[-0.0560,  0.2359, -0.0028],\n",
      "          [ 0.0132, -0.0720,  0.1126],\n",
      "          [-0.0146,  0.1242,  0.0077]]],\n",
      "\n",
      "\n",
      "        [[[-0.0306,  0.2027, -0.0162],\n",
      "          [-0.0180, -0.0123,  0.0308],\n",
      "          [-0.0058,  0.0858, -0.0088]],\n",
      "\n",
      "         [[-0.0069,  0.0215, -0.0265],\n",
      "          [ 0.1045, -0.0269,  0.0047],\n",
      "          [ 0.0100,  0.0439, -0.0486]]]], device='cuda:0')\n",
      "tensor([-0.1674, -0.4186,  0.2886], device='cuda:0')\n",
      "tensor([-0.1007,  0.0516,  0.4106], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "y_ref.backward(dy, retain_graph=True)\n",
    "dx_ref, dg_ref, db_ref = [tensor.grad.clone() for tensor in [x, gain, bias]]\n",
    "x.grad, gain.grad, bias.grad = None, None, None  # Reset gradients for use later\n",
    "\n",
    "print(dx_ref)\n",
    "print(dg_ref)\n",
    "print(db_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare with the Triton result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.0085,  0.0178, -0.0186],\n",
      "          [ 0.1165, -0.0835,  0.1077],\n",
      "          [-0.1970,  0.2153,  0.0060]],\n",
      "\n",
      "         [[-0.0560,  0.2359, -0.0028],\n",
      "          [ 0.0132, -0.0720,  0.1126],\n",
      "          [-0.0146,  0.1242,  0.0077]]],\n",
      "\n",
      "\n",
      "        [[[-0.0306,  0.2027, -0.0162],\n",
      "          [-0.0180, -0.0123,  0.0308],\n",
      "          [-0.0058,  0.0858, -0.0088]],\n",
      "\n",
      "         [[-0.0069,  0.0215, -0.0265],\n",
      "          [ 0.1045, -0.0269,  0.0047],\n",
      "          [ 0.0100,  0.0439, -0.0486]]]], device='cuda:0')\n",
      "tensor([-0.1674, -0.4186,  0.2886], device='cuda:0')\n",
      "tensor([-0.1007,  0.0516,  0.4106], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "y_tri.backward(dy, retain_graph=True)\n",
    "dx_tri, dg_tri, db_tri = [tensor.grad.clone() for tensor in [x, gain, bias]]\n",
    "\n",
    "print(dx_tri)\n",
    "print(dg_tri)\n",
    "print(db_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Triton and Baseline match\n"
     ]
    }
   ],
   "source": [
    "if (\n",
    "    torch.allclose(dx_tri, dx_ref, atol=1e-3)\n",
    "    and torch.allclose(dg_tri, dg_ref, atol=1e-3)\n",
    "    and torch.allclose(db_tri, db_ref, atol=1e-3)\n",
    "):\n",
    "    print(\"✅ Triton and Baseline match\")\n",
    "else:\n",
    "    raise ValueError(\"❌ Triton and Baseline differ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform benchmarking for both the forward and backward passes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define the benchmark configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=[\"M\", \"N\"],  # Argument names to use as an x-axis for the plot\n",
    "        x_vals=[64 * i for i in range(1, 17)],  # Different possible values for `x_name`\n",
    "        line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "        # Possible values for `line_arg`\n",
    "        line_vals=[\"baseline\", \"triton\"],  # Label name for the lines\n",
    "        line_names=[\"Baseline\", \"Triton\"],  # Line styles\n",
    "        styles=[(\"green\", \"-\"), (\"blue\", \"-\")],\n",
    "        ylabel=\"GB/s\",  # Label name for the y-axis\n",
    "        plot_name=f\"quant-rms-norm-{mode}\",  # Name for the plot, used also as a file name for saving the plot.\n",
    "        args={\"mode\": mode},\n",
    "    )\n",
    "    for mode in [\"forward\", \"backward\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the actual benchmark tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial when shape = (4, 4, 64, 64) for baseline for forward pass\n",
      "Trial when shape = (4, 4, 64, 64) for triton for forward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_fwd_kernel finished after 1.26s; best config selected: num_warps: 1, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 128, 128) for baseline for forward pass\n",
      "Trial when shape = (4, 4, 128, 128) for triton for forward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_fwd_kernel finished after 1.26s; best config selected: num_warps: 1, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 192, 192) for baseline for forward pass\n",
      "Trial when shape = (4, 4, 192, 192) for triton for forward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_fwd_kernel finished after 1.28s; best config selected: num_warps: 2, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 256, 256) for baseline for forward pass\n",
      "Trial when shape = (4, 4, 256, 256) for triton for forward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_fwd_kernel finished after 1.28s; best config selected: num_warps: 1, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 320, 320) for baseline for forward pass\n",
      "Trial when shape = (4, 4, 320, 320) for triton for forward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_fwd_kernel finished after 1.30s; best config selected: num_warps: 4, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 384, 384) for baseline for forward pass\n",
      "Trial when shape = (4, 4, 384, 384) for triton for forward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_fwd_kernel finished after 1.30s; best config selected: num_warps: 4, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 448, 448) for baseline for forward pass\n",
      "Trial when shape = (4, 4, 448, 448) for triton for forward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_fwd_kernel finished after 1.33s; best config selected: num_warps: 4, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 512, 512) for baseline for forward pass\n",
      "Trial when shape = (4, 4, 512, 512) for triton for forward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_fwd_kernel finished after 1.32s; best config selected: num_warps: 4, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 576, 576) for baseline for forward pass\n",
      "Trial when shape = (4, 4, 576, 576) for triton for forward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_fwd_kernel finished after 1.32s; best config selected: num_warps: 2, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 640, 640) for baseline for forward pass\n",
      "Trial when shape = (4, 4, 640, 640) for triton for forward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_fwd_kernel finished after 1.31s; best config selected: num_warps: 2, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 704, 704) for baseline for forward pass\n",
      "Trial when shape = (4, 4, 704, 704) for triton for forward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_fwd_kernel finished after 1.30s; best config selected: num_warps: 1, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 768, 768) for baseline for forward pass\n",
      "Trial when shape = (4, 4, 768, 768) for triton for forward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_fwd_kernel finished after 1.30s; best config selected: num_warps: 1, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 832, 832) for baseline for forward pass\n",
      "Trial when shape = (4, 4, 832, 832) for triton for forward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_fwd_kernel finished after 1.32s; best config selected: num_warps: 8, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 896, 896) for baseline for forward pass\n",
      "Trial when shape = (4, 4, 896, 896) for triton for forward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_fwd_kernel finished after 1.33s; best config selected: num_warps: 1, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 960, 960) for baseline for forward pass\n",
      "Trial when shape = (4, 4, 960, 960) for triton for forward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_fwd_kernel finished after 1.35s; best config selected: num_warps: 1, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 1024, 1024) for baseline for forward pass\n",
      "Trial when shape = (4, 4, 1024, 1024) for triton for forward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_fwd_kernel finished after 1.37s; best config selected: num_warps: 8, num_ctas: 1, num_stages: 2, maxnreg: None;\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quant-rms-norm-forward:\n",
      "         M       N  Baseline     Triton\n",
      "0     64.0    64.0  4.654545  42.666665\n",
      "1    128.0   128.0  4.409042  68.266668\n",
      "2    192.0   192.0  1.110361  16.575540\n",
      "3    256.0   256.0  1.264393  16.633502\n",
      "4    320.0   320.0  1.840403  17.169685\n",
      "5    384.0   384.0  2.894927  16.218215\n",
      "6    448.0   448.0  2.871466  16.815014\n",
      "7    512.0   512.0  3.632816  24.200887\n",
      "8    576.0   576.0  3.518602  24.238457\n",
      "9    640.0   640.0  3.575669  24.190882\n",
      "10   704.0   704.0  4.943504  65.976571\n",
      "11   768.0   768.0  4.964347  67.733577\n",
      "12   832.0   832.0  4.951956  65.675900\n",
      "13   896.0   896.0  4.978272  67.463528\n",
      "14   960.0   960.0  4.943251  67.289720\n",
      "15  1024.0  1024.0  4.985243  68.427041\n",
      "Trial when shape = (4, 4, 64, 64) for baseline for backward pass\n",
      "Trial when shape = (4, 4, 64, 64) for triton for backward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_bwd_kernel finished after 1.89s; best config selected: num_warps: 1, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 128, 128) for baseline for backward pass\n",
      "Trial when shape = (4, 4, 128, 128) for triton for backward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_bwd_kernel finished after 1.76s; best config selected: num_warps: 1, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 192, 192) for baseline for backward pass\n",
      "Trial when shape = (4, 4, 192, 192) for triton for backward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_bwd_kernel finished after 1.82s; best config selected: num_warps: 8, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 256, 256) for baseline for backward pass\n",
      "Trial when shape = (4, 4, 256, 256) for triton for backward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_bwd_kernel finished after 1.33s; best config selected: num_warps: 8, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 320, 320) for baseline for backward pass\n",
      "Trial when shape = (4, 4, 320, 320) for triton for backward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_bwd_kernel finished after 1.94s; best config selected: num_warps: 16, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 384, 384) for baseline for backward pass\n",
      "Trial when shape = (4, 4, 384, 384) for triton for backward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_bwd_kernel finished after 1.92s; best config selected: num_warps: 8, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 448, 448) for baseline for backward pass\n",
      "Trial when shape = (4, 4, 448, 448) for triton for backward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_bwd_kernel finished after 1.30s; best config selected: num_warps: 4, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 512, 512) for baseline for backward pass\n",
      "Trial when shape = (4, 4, 512, 512) for triton for backward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_bwd_kernel finished after 1.31s; best config selected: num_warps: 2, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 576, 576) for baseline for backward pass\n",
      "Trial when shape = (4, 4, 576, 576) for triton for backward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_bwd_kernel finished after 2.06s; best config selected: num_warps: 4, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 640, 640) for baseline for backward pass\n",
      "Trial when shape = (4, 4, 640, 640) for triton for backward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_bwd_kernel finished after 2.07s; best config selected: num_warps: 1, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 704, 704) for baseline for backward pass\n",
      "Trial when shape = (4, 4, 704, 704) for triton for backward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_bwd_kernel finished after 1.31s; best config selected: num_warps: 1, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 768, 768) for baseline for backward pass\n",
      "Trial when shape = (4, 4, 768, 768) for triton for backward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_bwd_kernel finished after 1.33s; best config selected: num_warps: 1, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 832, 832) for baseline for backward pass\n",
      "Trial when shape = (4, 4, 832, 832) for triton for backward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_bwd_kernel finished after 1.35s; best config selected: num_warps: 1, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 896, 896) for baseline for backward pass\n",
      "Trial when shape = (4, 4, 896, 896) for triton for backward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_bwd_kernel finished after 1.37s; best config selected: num_warps: 1, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 960, 960) for baseline for backward pass\n",
      "Trial when shape = (4, 4, 960, 960) for triton for backward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_bwd_kernel finished after 1.37s; best config selected: num_warps: 1, num_ctas: 1, num_stages: 2, maxnreg: None;\n",
      "Trial when shape = (4, 4, 1024, 1024) for baseline for backward pass\n",
      "Trial when shape = (4, 4, 1024, 1024) for triton for backward pass\n",
      "Triton autotuning for function quant_rms_norm_nd_bwd_kernel finished after 1.26s; best config selected: num_warps: 1, num_ctas: 1, num_stages: 2, maxnreg: None;\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quant-rms-norm-backward:\n",
      "         M       N  Baseline     Triton\n",
      "0     64.0    64.0  5.314879  24.000000\n",
      "1    128.0   128.0  4.376068  54.857142\n",
      "2    192.0   192.0  1.463787  12.847583\n",
      "3    256.0   256.0  1.830751  15.302615\n",
      "4    320.0   320.0  2.398201  15.527699\n",
      "5    384.0   384.0  3.707160  17.604585\n",
      "6    448.0   448.0  4.914719  35.468428\n",
      "7    512.0   512.0  5.301122  34.097817\n",
      "8    576.0   576.0  5.532797  35.854756\n",
      "9    640.0   640.0  5.405025  36.005627\n",
      "10   704.0   704.0  6.925622  68.329412\n",
      "11   768.0   768.0  6.947825  68.947631\n",
      "12   832.0   832.0  6.950784  68.929452\n",
      "13   896.0   896.0  6.967760  69.017879\n",
      "14   960.0   960.0  6.949528  69.147661\n",
      "15  1024.0  1024.0  6.978597  69.325812\n"
     ]
    }
   ],
   "source": [
    "# ruff: noqa: E731\n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(M, N, provider, mode):\n",
    "    x_shape = (4, 4, M, N)\n",
    "    weight_shape = (x_shape[-1],)\n",
    "\n",
    "    print(f\"Trial when shape = {x_shape} for {provider} for {mode} pass\")\n",
    "\n",
    "    x = torch.rand(x_shape, device=\"cuda\", requires_grad=True)\n",
    "    gain = torch.rand(weight_shape, device=\"cuda\", requires_grad=True)\n",
    "    bias = torch.rand(weight_shape, device=\"cuda\", requires_grad=True)\n",
    "    dy = 0.1 * torch.randn_like(x)\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "\n",
    "    def y_fwd():\n",
    "        if provider == \"baseline\":\n",
    "            return quant_rms_norm_baseline(x, gain, bias, 1e-5)\n",
    "        if provider == \"triton\":\n",
    "            return quant_rms_norm_triton(x, gain, bias, 1e-5)\n",
    "\n",
    "    if mode == \"forward\":\n",
    "        gbps = lambda ms: 2 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(y_fwd, quantiles=quantiles)\n",
    "    else:  # Backward\n",
    "        y = y_fwd()\n",
    "        gbps = lambda ms: 3 * x.numel() * x.element_size() * 1e-9 / (ms * 1e-3)\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(\n",
    "            lambda: y.backward(dy, retain_graph=True), quantiles=quantiles, grad_to_none=[x]\n",
    "        )\n",
    "\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)\n",
    "\n",
    "\n",
    "# benchmark.run(show_plots=True, print_data=True)  # TODO: Re-enable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
