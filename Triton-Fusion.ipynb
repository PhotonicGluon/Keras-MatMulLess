{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fused Layers in Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/\n",
      "Requirement already satisfied: triton-nightly==3.0.0.post20240626041721 in /home/vscode/.cache/pypoetry/virtualenvs/keras-matmulless-b9IALFmu-py3.10/lib/python3.10/site-packages (3.0.0.post20240626041721)\n",
      "Requirement already satisfied: filelock in /home/vscode/.cache/pypoetry/virtualenvs/keras-matmulless-b9IALFmu-py3.10/lib/python3.10/site-packages (from triton-nightly==3.0.0.post20240626041721) (3.15.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/ triton-nightly==3.0.0.post20240626041721"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # This is to avoid conflicts with PyTorch and the Triton compatibility\n",
    "os.environ[\"TRITON_PRINT_AUTOTUNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the installed triton version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "\n",
    "assert triton.__version__ == \"3.0.0\", f\"Expected Triton to have a version of 3.0.0, but found {triton.__version__}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import other needed stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generic Autotune Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autotune_config():\n",
    "    return [\n",
    "        triton.Config({}, num_warps=1),\n",
    "        triton.Config({}, num_warps=2),\n",
    "        triton.Config({}, num_warps=4),\n",
    "        triton.Config({}, num_warps=8),\n",
    "        triton.Config({}, num_warps=16),\n",
    "        triton.Config({}, num_warps=32),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fused Layer Norm with Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to fuse the RMSNorm and the quantization into one layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: N803\n",
    "@triton.autotune(\n",
    "    configs=get_autotune_config(),\n",
    "    key=[\"N\", \"HAS_BIAS\"],\n",
    ")\n",
    "@triton.jit\n",
    "def quant_rms_norm_fwd_kernel(\n",
    "    # fmt: off\n",
    "    # Pointers to arrays\n",
    "    x_ptr, y_ptr, gain_ptr, bias_ptr, rstd_ptr,\n",
    "    # Strides\n",
    "    stride_x_row,  # How much to increase the pointer when moving by 1 row\n",
    "    stride_y_row,\n",
    "    # Some constants\n",
    "    N,  # Number of columns in X\n",
    "    EPSILON,  # To avoid division by zero\n",
    "    # Meta-parameters\n",
    "    BLOCK_SIZE_N: tl.constexpr,\n",
    "    HAS_GAIN: tl.constexpr,\n",
    "    HAS_BIAS: tl.constexpr\n",
    "    # fmt: on\n",
    "):\n",
    "    \"\"\"\n",
    "    Forward kernel.\n",
    "\n",
    "    Performs RMSNorm on ``X``, followed by 8-bit quantization.\n",
    "    \"\"\"\n",
    "\n",
    "    # Map the PID to the row of X that should be loaded\n",
    "    pid = tl.program_id(0)\n",
    "    base_x = pid * stride_x_row\n",
    "    offsets = tl.arange(0, BLOCK_SIZE_N)\n",
    "    mask = offsets < N\n",
    "    x = tl.load(x_ptr + base_x + offsets, mask=mask, other=0.0).to(tl.float32)  # Load in higher precision\n",
    "\n",
    "    # Compute reciprocal standard deviation (rstd)\n",
    "    x_bar = tl.where(offsets < N, x, 0.0)  # Masked `x` to avoid illegal access\n",
    "    variance = tl.sum(x_bar * x_bar, axis=0) / N\n",
    "    rstd = 1 / tl.sqrt(variance + EPSILON)\n",
    "    tl.store(rstd_ptr + pid, rstd)  # We add PID since that is the row that the rstd is corresponding to\n",
    "\n",
    "    # Normalize\n",
    "    x_hat = x * rstd\n",
    "\n",
    "    # Apply gain and bias\n",
    "    y = x_hat\n",
    "\n",
    "    if HAS_GAIN:\n",
    "        gain = tl.load(gain_ptr + offsets, mask=mask).to(tl.float32)\n",
    "        y = y * gain\n",
    "    if HAS_BIAS:\n",
    "        bias = tl.load(bias_ptr + offsets, mask=mask).to(tl.float32)\n",
    "        y = y + bias\n",
    "\n",
    "    # Apply 8-bit quantization\n",
    "    scale = 127.0 / tl.maximum(tl.max(tl.abs(y), 0), EPSILON)\n",
    "    y = tl.extra.cuda.libdevice.round(y * scale)  # TODO: This is CUDA only... can we generalize this?\n",
    "    y = tl.maximum(tl.minimum(y, 127), -128) / scale  # The nested max and min creates the clamp/clip function\n",
    "\n",
    "    # Write output\n",
    "    base_y = pid * stride_y_row\n",
    "    tl.store(y_ptr + base_y + offsets, y, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define the companion function that handles checking and allocation of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruff: noqa: N806, S101\n",
    "def quant_rms_norm_fwd(x, gain, bias, epsilon):\n",
    "    \"\"\"\n",
    "    Forward pass.\n",
    "\n",
    "    Performs RMSNorm on ``X``, followed by 8-bit quantization.\n",
    "\n",
    "    Requires CUDA.\n",
    "    \"\"\"\n",
    "\n",
    "    assert x.ndim == 2  # TODO: Support other ndim values?\n",
    "\n",
    "    # Get dimensions\n",
    "    M, N = x.shape\n",
    "\n",
    "    # Validate that the input is OK\n",
    "    assert x.stride(-1) == 1\n",
    "\n",
    "    if gain is not None:\n",
    "        assert gain.shape == (N,)\n",
    "        assert gain.stride(-1) == 1\n",
    "    if bias is not None:\n",
    "        assert bias.shape == (N,)\n",
    "        assert bias.stride(-1) == 1\n",
    "\n",
    "    # Allocate output\n",
    "    y = torch.empty_like(x, dtype=x.dtype)\n",
    "    rstd = torch.empty((M,), dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "    # Enqueue fused kernel if less than 64KiB per feature\n",
    "    MAX_FUSED_SIZE = 65536 // x.element_size()\n",
    "    BLOCK_SIZE_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))\n",
    "    if N > BLOCK_SIZE_N:\n",
    "        raise RuntimeError(\"This layer norm doesn't support feature dim >= 64KiB.\")\n",
    "\n",
    "    # Run the kernel\n",
    "    with torch.cuda.device(x.device.index):\n",
    "        quant_rms_norm_fwd_kernel[(M,)](\n",
    "            # fmt: off\n",
    "            # Pointers to arrays\n",
    "            x, y, gain, bias, rstd,\n",
    "            # Strides\n",
    "            x.stride(0),\n",
    "            y.stride(0),\n",
    "            # Some constants\n",
    "            N,  # Number of columns in X\n",
    "            epsilon,  # To avoid division by zero\n",
    "            # Meta-parameters\n",
    "            BLOCK_SIZE_N,\n",
    "            gain is not None,\n",
    "            bias is not None\n",
    "            # fmt: on\n",
    "        )\n",
    "\n",
    "    # Return stuff\n",
    "    return y, rstd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First define the array that we want to normalize and quantize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26927402, 0.61529423, 0.09748146, 0.09273818, 0.73007226,\n",
       "        0.58209141, 0.60969096, 0.61006532],\n",
       "       [0.12066607, 0.24657084, 0.04623944, 0.30049028, 0.35141794,\n",
       "        0.13295607, 0.14754041, 0.03140163],\n",
       "       [0.61991843, 0.14574681, 0.26931555, 0.4847425 , 0.3966334 ,\n",
       "        0.88168223, 0.83791913, 0.5625612 ],\n",
       "       [0.87264191, 0.26286043, 0.58907885, 0.99076971, 0.69954819,\n",
       "        0.60982454, 0.37018737, 0.83690735],\n",
       "       [0.89161974, 0.01654753, 0.08784621, 0.01367872, 0.98187445,\n",
       "        0.14133196, 0.06238426, 0.98368333],\n",
       "       [0.61991595, 0.28636141, 0.08490172, 0.0382211 , 0.48337241,\n",
       "        0.86436947, 0.33873519, 0.79784515],\n",
       "       [0.35204787, 0.90411859, 0.0203531 , 0.27302275, 0.32587877,\n",
       "        0.52114029, 0.52831412, 0.857042  ],\n",
       "       [0.33801928, 0.81751108, 0.124455  , 0.72949503, 0.08025578,\n",
       "        0.67488853, 0.30774964, 0.32175416]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.random((8, 8))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the baseline result. We will be using another backend for the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_mml.layers import QuantRMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52924716, 1.2048818 , 0.19142982, 0.18016924, 1.4300933 ,\n",
       "        1.1373184 , 1.1936212 , 1.1936212 ],\n",
       "       [0.5979084 , 1.2094055 , 0.23101005, 1.4811821 , 1.725781  ,\n",
       "        0.65226364, 0.7202078 , 0.1494771 ],\n",
       "       [1.0702566 , 0.25253245, 0.46898884, 0.8417748 , 0.6854452 ,\n",
       "        1.52722   , 1.4550679 , 0.9740537 ],\n",
       "       [1.2582126 , 0.38195738, 0.85378706, 1.4267231 , 1.0110636 ,\n",
       "        0.87625515, 0.5279999 , 1.2020423 ],\n",
       "       [1.5168372 , 0.02637978, 0.14508878, 0.02637978, 1.6751158 ,\n",
       "        0.237418  , 0.10551911, 1.6751158 ],\n",
       "       [1.1791687 , 0.5442317 , 0.15549478, 0.07774739, 0.92001075,\n",
       "        1.645653  , 0.6478949 , 1.5160741 ],\n",
       "       [0.63585424, 1.6480303 , 0.03892985, 0.49311143, 0.59692436,\n",
       "        0.94729304, 0.96026963, 1.557194  ],\n",
       "       [0.6839782 , 1.6389667 , 0.24519974, 1.4582932 , 0.154863  ,\n",
       "        1.3550512 , 0.619452  , 0.6452625 ]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_output = QuantRMSNorm()(x)\n",
    "baseline_output = np.array(baseline_output)\n",
    "baseline_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare that with the Triton result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_torch = torch.tensor(x, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52923703, 1.20485878, 0.19142616, 0.1801658 , 1.43006599,\n",
       "        1.13729656, 1.19359839, 1.19359839],\n",
       "       [0.59783626, 1.20925975, 0.2309822 , 1.48100352, 1.72557282,\n",
       "        0.65218502, 0.72012097, 0.14945906],\n",
       "       [1.07024038, 0.25252864, 0.46898174, 0.84176213, 0.68543488,\n",
       "        1.527197  , 1.45504594, 0.97403902],\n",
       "       [1.25819969, 0.38195348, 0.85377836, 1.42670858, 1.01105332,\n",
       "        0.87624621, 0.52799451, 1.20203006],\n",
       "       [1.51681519, 0.0263794 , 0.14508668, 0.0263794 , 1.67509162,\n",
       "        0.23741455, 0.10551758, 1.67509162],\n",
       "       [1.17914748, 0.54422194, 0.15549198, 0.07774599, 0.91999424,\n",
       "        1.64562345, 0.64788324, 1.51604676],\n",
       "       [0.63584358, 1.64800274, 0.0389292 , 0.49310318, 0.59691441,\n",
       "        0.94727719, 0.9602536 , 1.55716801],\n",
       "       [0.68396449, 1.63893378, 0.24519482, 1.45826387, 0.15485989,\n",
       "        1.35502398, 0.61943954, 0.64524955]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_output, triton_rstd = quant_rms_norm_fwd(x_torch, None, None, 1e-5)\n",
    "triton_output = np.array(triton_output.cpu())\n",
    "triton_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Triton and Baseline match\n"
     ]
    }
   ],
   "source": [
    "if np.allclose(triton_output, baseline_output, atol=1e-3):\n",
    "    print(\"✅ Triton and Baseline match\")\n",
    "else:\n",
    "    raise ValueError(\"❌ Triton and Baseline differ\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-matmulless-b9IALFmu-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
