{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdrF5MIoMXX8"
   },
   "source": [
    "# Ternary Multiplication in Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGNuCpfuMpAU"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DAdIYpUMjyD"
   },
   "source": [
    "Check the installed triton version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hNPo7UMLMlVY"
   },
   "outputs": [],
   "source": [
    "import triton\n",
    "assert triton.__version__ == \"3.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zh04HhvLMznK"
   },
   "source": [
    "Import other needed stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uglYrA26M0vg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton.language as tl\n",
    "from jaxtyping import Float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pI2dDdjsbPnB"
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mzcLVuDLM30m"
   },
   "outputs": [],
   "source": [
    "def get_current_target():\n",
    "    return triton.runtime.driver.active.get_current_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "57uA2WypcCAR"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "def is_cuda():\n",
    "    current_target = get_current_target()\n",
    "    if current_target.backend != \"cuda\":\n",
    "        return False\n",
    "\n",
    "    if current_target.arch < 70:  # CUDA compute capacity is below 7.0, which is minimum 'stable' supported\n",
    "        warnings.warn(\"Compute capcity of CUDA device is below 7.0. The Triton compilation may fail terribly!\")\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Xs_RIxorbvP3"
   },
   "outputs": [],
   "source": [
    "# def is_hip_mi200():\n",
    "#     target = triton.runtime.driver.active.get_current_target()\n",
    "#     return target.backend == \"hip\" and target.arch == \"gfx90a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ex9yAjaFcFwd"
   },
   "outputs": [],
   "source": [
    "# is_hip_mi200()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "flaq-3Gez1dl"
   },
   "outputs": [],
   "source": [
    "def get_cuda_autotune_config():\n",
    "    return [\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_M\": 128,\n",
    "                \"BLOCK_SIZE_N\": 128,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_M\": 128,\n",
    "                \"BLOCK_SIZE_N\": 64,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_M\": 64,\n",
    "                \"BLOCK_SIZE_N\": 128,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_M\": 128,\n",
    "                \"BLOCK_SIZE_N\": 32,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_M\": 32,\n",
    "                \"BLOCK_SIZE_N\": 128,\n",
    "            },\n",
    "            num_stages=4,\n",
    "            num_warps=4,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_M\": 64,\n",
    "                \"BLOCK_SIZE_N\": 32,\n",
    "            },\n",
    "            num_stages=5,\n",
    "            num_warps=2,\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\n",
    "                \"BLOCK_SIZE_M\": 32,\n",
    "                \"BLOCK_SIZE_N\": 64,\n",
    "            },\n",
    "            num_stages=5,\n",
    "            num_warps=2,\n",
    "        ),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gkI9jzg80Fkt"
   },
   "outputs": [],
   "source": [
    "def get_autotune_config():\n",
    "    if is_cuda():\n",
    "        return get_cuda_autotune_config()\n",
    "    else:\n",
    "        raise ValueError(\"Not on CUDA... can't use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main ternary multiplication kernel.\n",
    "\n",
    "The rough pseudocode algorithm is as follows.\n",
    "```python\n",
    "# Do in parallel\n",
    "for n in range(0, N, BLOCK_SIZE_N):\n",
    "    acc = zeros((BLOCK_SIZE_N,), dtype=float32)\n",
    "    for m in range(0, M, BLOCK_SIZE_M):\n",
    "        x_block = x[m : m+BLOCK_SIZE_M]\n",
    "        w_block = w[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N]\n",
    "        \n",
    "        # Since `w` is ternary, we only really care about the sign of the element in the array, and so\n",
    "        # we just need to perform two conditional checks\n",
    "        elems_to_sum = tl.where(w_block > 0, x_block, tl.where(w_block < 0, -x_block, tl.zeros_like(x_block)))\n",
    "        acc += tl.sum(elems_to_sum)  # Sum along the M direction\n",
    "\n",
    "    acc = acc / scale\n",
    "    z[n : n+BLOCK_SIZE_N] = acc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35864/1541291054.py:10: UserWarning: Compute capcity of CUDA device is below 7.0. The Triton compilation may fail terribly!\n",
      "  warnings.warn(\"Compute capcity of CUDA device is below 7.0. The Triton compilation may fail terribly!\")\n"
     ]
    }
   ],
   "source": [
    "# ruff: noqa: N803, PLR2044\n",
    "@triton.autotune(\n",
    "    configs=get_autotune_config(),\n",
    "    key=[\"M\", \"N\"],\n",
    ")\n",
    "@triton.jit\n",
    "def ternary_mul_kernel(\n",
    "    # Pointers to matrices\n",
    "    x_ptr,\n",
    "    w_ptr,\n",
    "    z_ptr,\n",
    "    # Scaling factor\n",
    "    scale,\n",
    "    # `W` matrix dimensions\n",
    "    M,\n",
    "    N,\n",
    "    # The stride variables represent how much to increase the ptr by when moving by 1\n",
    "    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
    "    # by to get the element one row down (A has M rows).\n",
    "    stride_xm,\n",
    "    stride_wm,\n",
    "    stride_wn,\n",
    "    # Meta-parameters\n",
    "    BLOCK_SIZE_M: tl.constexpr,\n",
    "    BLOCK_SIZE_N: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Kernel for computing the ternary multiplication\n",
    "        z = xW\n",
    "    `x` has shape `(1, M)`, `W` has shape `(M, N)`, and `z` has shape `(1, N)`.\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Map `pid` to the block of `z` that it should compute.\n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Create pointers for the first blocks of `x` and `W`.\n",
    "    # We will advance this pointer as we move in the `M` direction and accumulate.\n",
    "    # - `x_ptrs` is a block of `BLOCK_SIZE_M` pointers\n",
    "    # - `w_ptrs` is a block of pointers with shape `(BLOCK_SIZE_M, BLOCK_SIZE_N)`\n",
    "    offs_m = tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_n = (pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N  # Guard against wrong offsets\n",
    "    x_ptrs = x_ptr + offs_m\n",
    "    w_ptrs = w_ptr + (offs_m[:, None] * stride_wm + offs_n[None, :] * stride_wn)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the `z` vector.\n",
    "    # We accumulate into a block of `BLOCK_SIZE_N` elements of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n",
    "    for m in range(0, tl.cdiv(M, BLOCK_SIZE_M)):\n",
    "        # Load the next block of `x` and `W`, generate a mask by checking along `M`.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        x = tl.load(x_ptrs, mask=offs_m < M - m * BLOCK_SIZE_M, other=0.0)[:, None]  # Force broadcast to correct shape here\n",
    "        w = tl.load(w_ptrs, mask=offs_m[:, None] < M - m * BLOCK_SIZE_M, other=0.0)\n",
    "\n",
    "        # Since `w` is ternary, we only really care about the sign of the element in the array, and so\n",
    "        # we just need to perform two conditional checks\n",
    "        elements_to_sum = tl.where(w > 0, x, tl.where(w < 0, -x, tl.zeros_like(x)))\n",
    "        accumulator = accumulator + tl.sum(elements_to_sum, axis=0)  # Sum along the `M` direction\n",
    "\n",
    "        # Advance the ptrs to the next `M` block.\n",
    "        x_ptrs += BLOCK_SIZE_M * stride_xm\n",
    "        w_ptrs += BLOCK_SIZE_M * stride_wm\n",
    "\n",
    "    accumulator = accumulator / scale\n",
    "    z = accumulator  # TODO: Do we want to reduce to FP16?\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output vector `z` with masks.\n",
    "    offs_z = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    z_ptrs = z_ptr + offs_z\n",
    "    z_mask = offs_z < N\n",
    "    tl.store(z_ptrs, z, mask=z_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xexLN2Dz0GM7"
   },
   "source": [
    "We can now create a convenience wrapper function that only takes two input tensors, and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-MqHjzD30GeM"
   },
   "outputs": [],
   "source": [
    "# ruff: noqa: E731\n",
    "def ternary_mul(x, w, scale):\n",
    "    # Check constraints.\n",
    "    assert len(x) == w.shape[0], \"Incompatible dimensions\"\n",
    "    assert x.is_contiguous(), \"x must be contiguous\"\n",
    "\n",
    "    assert x.is_cuda and w.is_cuda\n",
    "\n",
    "    # Get dimensions\n",
    "    M, N = w.shape\n",
    "\n",
    "    # Allocate output\n",
    "    z = torch.empty((N,), device=x.device, dtype=torch.float32)  # TODO: Change precision?\n",
    "\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(N, META[\"BLOCK_SIZE_N\"]),)\n",
    "    ternary_mul_kernel[grid](\n",
    "        x, w, z,  #\n",
    "        scale,  #\n",
    "        M, N,  #\n",
    "        x.stride(0),  #\n",
    "        w.stride(0), w.stride(1)\n",
    "    )\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QC4g_Sb8ggMk"
   },
   "source": [
    "TESTING CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vX7hI8CV0mFX"
   },
   "outputs": [],
   "source": [
    "X_LEN = 256  # x is the 1D vector\n",
    "W_LEN = 256  # W is the quantized weights matrix\n",
    "W_SIZE = (X_LEN, W_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_QyszNe03Sr",
    "outputId": "fdf97a5a-9589-4856-e2c5-d2765fa244c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0c98143db0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YvvBIRqtgk1r"
   },
   "outputs": [],
   "source": [
    "x = torch.rand(X_LEN, device=\"cuda\", dtype=torch.float32)\n",
    "w = torch.tensor([-1., 0., 1.], device=\"cuda\", dtype=torch.float32)[torch.randint(2, W_SIZE)]\n",
    "scale = torch.rand(1, dtype=torch.float32).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dpUvXBwD05_E"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-951.4594, -810.9307, -814.1393, -753.9046, -771.3155, -788.1216,\n",
       "        -772.3747, -750.3213, -766.9465, -775.3512, -796.5271, -740.3124,\n",
       "        -743.4036, -764.6676, -699.1962, -728.9562, -786.4343, -858.7490,\n",
       "        -760.0906, -839.8545, -789.4946, -795.4908, -754.4786, -771.1029,\n",
       "        -783.6049, -834.9073, -931.8519, -694.6861, -695.2032, -707.0848,\n",
       "        -848.7382, -729.0391, -769.3309, -748.5757, -834.0535, -869.5665,\n",
       "        -803.8021, -884.6586, -680.9594, -903.9478, -761.8308, -839.1580,\n",
       "        -803.7127, -718.6333, -751.7179, -751.2381, -769.1642, -787.6885,\n",
       "        -768.6287, -815.1701, -668.5592, -739.2564, -811.2242, -763.2260,\n",
       "        -832.9089, -802.8115, -761.8378, -714.6352, -771.6232, -756.2714,\n",
       "        -863.0844, -838.2726, -721.9910, -765.3901, -799.4003, -860.3110,\n",
       "        -921.7515, -778.2712, -853.6852, -721.2637, -808.6849, -811.3694,\n",
       "        -699.9083, -809.1854, -914.3477, -809.1392, -791.8344, -796.9955,\n",
       "        -751.8312, -731.2245, -900.7673, -829.6279, -696.7488, -814.0972,\n",
       "        -745.0986, -840.6050, -721.4727, -811.9926, -740.4084, -884.6636,\n",
       "        -729.5006, -815.8346, -822.4435, -823.6513, -889.7301, -820.0322,\n",
       "        -752.2415, -840.0404, -703.8918, -815.4541, -686.0480, -681.2137,\n",
       "        -796.1993, -768.0498, -972.2722, -751.6311, -738.8566, -696.0828,\n",
       "        -826.9047, -784.3642, -834.1297, -739.7773, -766.2885, -826.3129,\n",
       "        -737.8804, -797.8938, -796.2531, -852.1828, -639.4240, -891.1442,\n",
       "        -865.8923, -782.4530, -799.7996, -784.1414, -700.7862, -822.1578,\n",
       "        -801.3296, -675.0355, -764.7386, -782.5708, -829.5156, -692.1326,\n",
       "        -787.8448, -693.6796, -846.4279, -744.8901, -801.2891, -837.5233,\n",
       "        -701.9556, -744.2525, -654.8667, -826.9344, -846.3845, -742.9725,\n",
       "        -767.7625, -862.4980, -726.5882, -695.3724, -717.1807, -847.3315,\n",
       "        -773.2690, -727.4025, -759.6781, -827.4999, -836.4473, -683.4453,\n",
       "        -843.7963, -834.5532, -945.6599, -798.1615, -783.8471, -716.9811,\n",
       "        -763.6321, -798.4788, -790.1544, -813.0567, -839.4808, -744.3832,\n",
       "        -754.0511, -744.0473, -864.3831, -862.7405, -753.4977, -863.2749,\n",
       "        -716.7770, -852.2720, -787.9136, -806.1100, -804.4357, -789.5510,\n",
       "        -713.6313, -738.0132, -801.2191, -860.4421, -806.0908, -745.6053,\n",
       "        -838.1174, -839.5273, -714.5489, -857.8897, -793.2915, -807.9446,\n",
       "        -796.9210, -794.3760, -834.6567, -759.6036, -704.5126, -814.5659,\n",
       "        -724.5497, -764.7427, -865.2620, -822.6932, -820.5858, -840.5030,\n",
       "        -819.2311, -816.2524, -741.2001, -815.1117, -804.2134, -787.5126,\n",
       "        -809.6976, -845.8332, -773.0093, -832.2064, -883.6086, -782.1836,\n",
       "        -744.2174, -796.0397, -798.9742, -724.2285, -778.5416, -883.7806,\n",
       "        -762.7258, -765.0425, -815.3953, -745.8015, -841.2291, -722.5185,\n",
       "        -926.7850, -922.4192, -807.0449, -759.0336, -747.6671, -734.6192,\n",
       "        -885.3647, -805.5819, -688.4378, -693.0744, -869.5692, -846.7794,\n",
       "        -788.2941, -791.1606, -773.1097, -861.0558, -719.6265, -776.2618,\n",
       "        -812.7778, -777.9195, -747.3929, -802.8259, -773.2090, -774.7262,\n",
       "        -739.1203, -858.7456, -881.1325, -801.0305], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_output = torch.matmul(x, w) / scale\n",
    "torch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FyLkFEHg1EPX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-951.4592, -810.9307, -814.1393, -753.9047, -771.3155, -788.1217,\n",
       "        -772.3747, -750.3212, -766.9465, -775.3511, -796.5271, -740.3126,\n",
       "        -743.4036, -764.6675, -699.1962, -728.9562, -786.4343, -858.7490,\n",
       "        -760.0906, -839.8545, -789.4948, -795.4908, -754.4785, -771.1029,\n",
       "        -783.6047, -834.9072, -931.8519, -694.6860, -695.2033, -707.0848,\n",
       "        -848.7383, -729.0392, -769.3309, -748.5757, -834.0536, -869.5665,\n",
       "        -803.8021, -884.6586, -680.9593, -903.9476, -761.8307, -839.1580,\n",
       "        -803.7127, -718.6334, -751.7179, -751.2381, -769.1642, -787.6886,\n",
       "        -768.6287, -815.1701, -668.5591, -739.2564, -811.2243, -763.2259,\n",
       "        -832.9091, -802.8115, -761.8378, -714.6352, -771.6231, -756.2714,\n",
       "        -863.0844, -838.2726, -721.9910, -765.3901, -799.4003, -860.3109,\n",
       "        -921.7516, -778.2714, -853.6852, -721.2637, -808.6850, -811.3694,\n",
       "        -699.9083, -809.1855, -914.3478, -809.1391, -791.8344, -796.9955,\n",
       "        -751.8312, -731.2245, -900.7673, -829.6279, -696.7488, -814.0972,\n",
       "        -745.0986, -840.6049, -721.4728, -811.9927, -740.4083, -884.6634,\n",
       "        -729.5006, -815.8347, -822.4435, -823.6512, -889.7301, -820.0322,\n",
       "        -752.2415, -840.0404, -703.8917, -815.4540, -686.0480, -681.2137,\n",
       "        -796.1993, -768.0498, -972.2722, -751.6311, -738.8564, -696.0828,\n",
       "        -826.9047, -784.3643, -834.1297, -739.7773, -766.2885, -826.3129,\n",
       "        -737.8804, -797.8939, -796.2531, -852.1828, -639.4240, -891.1442,\n",
       "        -865.8925, -782.4529, -799.7997, -784.1415, -700.7863, -822.1577,\n",
       "        -801.3296, -675.0354, -764.7386, -782.5707, -829.5156, -692.1324,\n",
       "        -787.8448, -693.6795, -846.4280, -744.8901, -801.2891, -837.5234,\n",
       "        -701.9556, -744.2526, -654.8667, -826.9344, -846.3846, -742.9723,\n",
       "        -767.7625, -862.4981, -726.5883, -695.3724, -717.1807, -847.3317,\n",
       "        -773.2690, -727.4025, -759.6781, -827.4999, -836.4472, -683.4453,\n",
       "        -843.7963, -834.5533, -945.6600, -798.1614, -783.8471, -716.9810,\n",
       "        -763.6322, -798.4790, -790.1544, -813.0568, -839.4809, -744.3833,\n",
       "        -754.0511, -744.0473, -864.3831, -862.7405, -753.4977, -863.2749,\n",
       "        -716.7771, -852.2720, -787.9135, -806.1099, -804.4357, -789.5510,\n",
       "        -713.6315, -738.0133, -801.2189, -860.4421, -806.0908, -745.6053,\n",
       "        -838.1174, -839.5273, -714.5490, -857.8897, -793.2915, -807.9447,\n",
       "        -796.9208, -794.3762, -834.6566, -759.6036, -704.5125, -814.5659,\n",
       "        -724.5497, -764.7428, -865.2618, -822.6932, -820.5858, -840.5030,\n",
       "        -819.2311, -816.2525, -741.2003, -815.1118, -804.2135, -787.5125,\n",
       "        -809.6976, -845.8333, -773.0093, -832.2064, -883.6086, -782.1835,\n",
       "        -744.2173, -796.0397, -798.9742, -724.2285, -778.5415, -883.7807,\n",
       "        -762.7258, -765.0425, -815.3953, -745.8015, -841.2291, -722.5185,\n",
       "        -926.7851, -922.4193, -807.0449, -759.0336, -747.6670, -734.6192,\n",
       "        -885.3647, -805.5822, -688.4378, -693.0745, -869.5692, -846.7792,\n",
       "        -788.2939, -791.1605, -773.1097, -861.0559, -719.6265, -776.2619,\n",
       "        -812.7776, -777.9195, -747.3930, -802.8259, -773.2090, -774.7262,\n",
       "        -739.1203, -858.7455, -881.1325, -801.0305], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_output = ternary_mul(x, w, scale)\n",
    "triton_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Triton and Torch match\n"
     ]
    }
   ],
   "source": [
    "if torch.allclose(triton_output, torch_output, atol=1e-3):\n",
    "    print(\"✅ Triton and Torch match\")\n",
    "else:\n",
    "    raise ValueError(\"❌ Triton and Torch differ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BENCHMARK CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial when M =  128 for cublas\n",
      "Trial when M =  128 for triton\n",
      "Trial when M =  256 for cublas\n",
      "Trial when M =  256 for triton\n",
      "Trial when M =  384 for cublas\n",
      "Trial when M =  384 for triton\n",
      "Trial when M =  512 for cublas\n",
      "Trial when M =  512 for triton\n",
      "Trial when M =  640 for cublas\n",
      "Trial when M =  640 for triton\n",
      "Trial when M =  768 for cublas\n",
      "Trial when M =  768 for triton\n",
      "Trial when M =  896 for cublas\n",
      "Trial when M =  896 for triton\n",
      "Trial when M =  1024 for cublas\n",
      "Trial when M =  1024 for triton\n",
      "Trial when M =  1152 for cublas\n",
      "Trial when M =  1152 for triton\n",
      "Trial when M =  1280 for cublas\n",
      "Trial when M =  1280 for triton\n",
      "Trial when M =  1408 for cublas\n",
      "Trial when M =  1408 for triton\n",
      "Trial when M =  1536 for cublas\n",
      "Trial when M =  1536 for triton\n",
      "Trial when M =  1664 for cublas\n",
      "Trial when M =  1664 for triton\n",
      "Trial when M =  1792 for cublas\n",
      "Trial when M =  1792 for triton\n",
      "Trial when M =  1920 for cublas\n",
      "Trial when M =  1920 for triton\n",
      "Trial when M =  2048 for cublas\n",
      "Trial when M =  2048 for triton\n",
      "Trial when M =  2176 for cublas\n",
      "Trial when M =  2176 for triton\n",
      "Trial when M =  2304 for cublas\n",
      "Trial when M =  2304 for triton\n",
      "Trial when M =  2432 for cublas\n",
      "Trial when M =  2432 for triton\n",
      "Trial when M =  2560 for cublas\n",
      "Trial when M =  2560 for triton\n",
      "Trial when M =  2688 for cublas\n",
      "Trial when M =  2688 for triton\n",
      "Trial when M =  2816 for cublas\n",
      "Trial when M =  2816 for triton\n",
      "Trial when M =  2944 for cublas\n",
      "Trial when M =  2944 for triton\n",
      "Trial when M =  3072 for cublas\n",
      "Trial when M =  3072 for triton\n",
      "Trial when M =  3200 for cublas\n",
      "Trial when M =  3200 for triton\n",
      "Trial when M =  3328 for cublas\n",
      "Trial when M =  3328 for triton\n"
     ]
    }
   ],
   "source": [
    "ref_lib = \"cuBLAS\"\n",
    "\n",
    "configs = [\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=[\"M\", \"N\"],  # Argument names to use as an x-axis for the plot\n",
    "        x_vals=[128 * i for i in range(1, 31)],  # Different possible values for `x_name`\n",
    "        line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "        # Possible values for `line_arg`\n",
    "        line_vals=[ref_lib.lower(), \"triton\"],  # Label name for the lines\n",
    "        line_names=[ref_lib, \"Triton\"],  # Line styles\n",
    "        styles=[(\"green\", \"-\"), (\"blue\", \"-\")],\n",
    "        ylabel=\"TFLOPS\",  # Label name for the y-axis\n",
    "        plot_name=\"ternary-mul-performance\",  # Name for the plot, used also as a file name for saving the plot.\n",
    "        args={},\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(M, N, provider):\n",
    "    print(\"Trial when M = \", M, \"for\", provider)\n",
    "    x = torch.rand(M, device=\"cuda\")\n",
    "    w = torch.tensor([-1., 0., 1.], device=\"cuda\")[torch.randint(2, (M, N))]\n",
    "    scale = 0.5\n",
    "    \n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == ref_lib.lower():\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(x, w) / scale, quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: ternary_mul(x, w, scale), quantiles=quantiles)\n",
    "    perf = lambda ms: 2 * M * N * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
