{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdrF5MIoMXX8"
   },
   "source": [
    "# Ternary Multiplication in Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGNuCpfuMpAU"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/\n",
      "Requirement already satisfied: triton-nightly==3.0.0.post20240626041721 in /home/vscode/.cache/pypoetry/virtualenvs/keras-matmulless-b9IALFmu-py3.10/lib/python3.10/site-packages (3.0.0.post20240626041721)\n",
      "Requirement already satisfied: filelock in /home/vscode/.cache/pypoetry/virtualenvs/keras-matmulless-b9IALFmu-py3.10/lib/python3.10/site-packages (from triton-nightly==3.0.0.post20240626041721) (3.15.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/Triton-Nightly/pypi/simple/ triton-nightly==3.0.0.post20240626041721"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TRITON_PRINT_AUTOTUNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DAdIYpUMjyD"
   },
   "source": [
    "Check the installed triton version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hNPo7UMLMlVY"
   },
   "outputs": [],
   "source": [
    "import triton\n",
    "\n",
    "assert triton.__version__ == \"3.0.0\", f\"Expected Triton to have a version of 3.0.0, but found {triton.__version__}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zh04HhvLMznK"
   },
   "source": [
    "Import other needed stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uglYrA26M0vg"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import torch\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pI2dDdjsbPnB"
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "mzcLVuDLM30m"
   },
   "outputs": [],
   "source": [
    "def get_current_target():\n",
    "    return triton.runtime.driver.active.get_current_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "57uA2WypcCAR"
   },
   "outputs": [],
   "source": [
    "def is_cuda():\n",
    "    current_target = get_current_target()\n",
    "    if current_target.backend != \"cuda\":\n",
    "        return False\n",
    "\n",
    "    if current_target.arch < 70:  # CUDA compute capacity is below 7.0, which is minimum 'stable' supported by Triton\n",
    "        warnings.warn(\n",
    "            \"Compute capacity of CUDA device is below 7.0. The Triton compilation may fail terribly!\", stacklevel=1\n",
    "        )\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Ternary Multiplication Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, define the autotune config for the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "flaq-3Gez1dl"
   },
   "outputs": [],
   "source": [
    "# def _get_autotune_config_2d():\n",
    "#     return [\n",
    "#         triton.Config(\n",
    "#             {\n",
    "#                 \"BLOCK_SIZE_M\": 256,\n",
    "#                 \"BLOCK_SIZE_N\": 256,\n",
    "#             },\n",
    "#             num_stages=4,\n",
    "#             num_warps=4,\n",
    "#         ),\n",
    "#         triton.Config(\n",
    "#             {\n",
    "#                 \"BLOCK_SIZE_M\": 128,\n",
    "#                 \"BLOCK_SIZE_N\": 128,\n",
    "#             },\n",
    "#             num_stages=4,\n",
    "#             num_warps=4,\n",
    "#         ),\n",
    "#         triton.Config(\n",
    "#             {\n",
    "#                 \"BLOCK_SIZE_M\": 128,\n",
    "#                 \"BLOCK_SIZE_N\": 64,\n",
    "#             },\n",
    "#             num_stages=4,\n",
    "#             num_warps=4,\n",
    "#         ),\n",
    "#         triton.Config(\n",
    "#             {\n",
    "#                 \"BLOCK_SIZE_M\": 64,\n",
    "#                 \"BLOCK_SIZE_N\": 128,\n",
    "#             },\n",
    "#             num_stages=4,\n",
    "#             num_warps=4,\n",
    "#         ),\n",
    "#         triton.Config(\n",
    "#             {\n",
    "#                 \"BLOCK_SIZE_M\": 128,\n",
    "#                 \"BLOCK_SIZE_N\": 32,\n",
    "#             },\n",
    "#             num_stages=4,\n",
    "#             num_warps=4,\n",
    "#         ),\n",
    "#         triton.Config(\n",
    "#             {\n",
    "#                 \"BLOCK_SIZE_M\": 32,\n",
    "#                 \"BLOCK_SIZE_N\": 128,\n",
    "#             },\n",
    "#             num_stages=4,\n",
    "#             num_warps=4,\n",
    "#         ),\n",
    "#         triton.Config(\n",
    "#             {\n",
    "#                 \"BLOCK_SIZE_M\": 64,\n",
    "#                 \"BLOCK_SIZE_N\": 32,\n",
    "#             },\n",
    "#             num_stages=5,\n",
    "#             num_warps=2,\n",
    "#         ),\n",
    "#         triton.Config(\n",
    "#             {\n",
    "#                 \"BLOCK_SIZE_M\": 32,\n",
    "#                 \"BLOCK_SIZE_N\": 64,\n",
    "#             },\n",
    "#             num_stages=5,\n",
    "#             num_warps=2,\n",
    "#         )\n",
    "#     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gkI9jzg80Fkt"
   },
   "outputs": [],
   "source": [
    "# def get_autotune_config_2d():\n",
    "#     if is_cuda():\n",
    "#         return _get_autotune_config_2d()\n",
    "#     else:\n",
    "#         raise ValueError(\"Not on CUDA... can't use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the Triton kernel. The rough pseudocode algorithm is as follows.\n",
    "```python\n",
    "# Do in parallel\n",
    "for n in range(0, N, BLOCK_SIZE_N):\n",
    "    acc = zeros((BLOCK_SIZE_N,), dtype=float32)\n",
    "    for m in range(0, M, BLOCK_SIZE_M):\n",
    "        x_block = x[m : m+BLOCK_SIZE_M]\n",
    "        w_block = w[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N]\n",
    "        \n",
    "        # Since `w` is ternary, we only really care about the sign of the element in the array, and so\n",
    "        # we just need to perform two conditional checks\n",
    "        elems_to_sum = tl.where(w_block > 0, x_block, tl.where(w_block < 0, -x_block, tl.zeros_like(x_block)))\n",
    "        acc += tl.sum(elems_to_sum)  # Sum along the M direction\n",
    "\n",
    "    acc = acc / scale\n",
    "    z[n : n+BLOCK_SIZE_N] = acc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ruff: noqa: N803, PLR2044\n",
    "# @triton.autotune(\n",
    "#     configs=get_autotune_config_2d(),\n",
    "#     key=[\"M\", \"N\"],\n",
    "# )\n",
    "# @triton.jit\n",
    "# def ternary_mul_2d_kernel(\n",
    "#     # Pointers to arrays\n",
    "#     x_ptr,\n",
    "#     w_ptr,\n",
    "#     z_ptr,\n",
    "#     # Scaling factor\n",
    "#     scale,\n",
    "#     # `W` matrix dimensions\n",
    "#     M,\n",
    "#     N,\n",
    "#     # The stride variables represent how much to increase the pointer by when moving by 1 element in a particular\n",
    "#     # dimension. E.g. `stride_wm` is how much to increase `w_ptr` by to get the element one row down (the `W` matrix\n",
    "#     # has `M` rows).\n",
    "#     stride_xm,\n",
    "#     stride_wm,\n",
    "#     stride_wn,\n",
    "#     # Meta-parameters\n",
    "#     BLOCK_SIZE_M: tl.constexpr,\n",
    "#     BLOCK_SIZE_N: tl.constexpr,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Kernel for computing the ternary multiplication\n",
    "#         z = xW\n",
    "#     `x` has shape `(M,)`, `W` has shape `(M, N)`, and `z` has shape `(N,)`.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # ----------------------------------------------------------\n",
    "#     # Create pointers for the first blocks of `x` and `W`.\n",
    "#     # We will advance this pointer as we move in the `M` direction and accumulate.\n",
    "#     # - `x_ptrs` is a block of `BLOCK_SIZE_M` pointers\n",
    "#     # - `w_ptrs` is a block of pointers with shape `(BLOCK_SIZE_M, BLOCK_SIZE_N)`\n",
    "#     pid_0 = tl.program_id(axis=0)\n",
    "\n",
    "#     offs_m = tl.arange(0, BLOCK_SIZE_M)\n",
    "#     offs_n = (pid_0 * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N  # Guard against wrong offsets\n",
    "\n",
    "#     x_ptrs = x_ptr + offs_m\n",
    "#     w_ptrs = w_ptr + (offs_m[:, None] * stride_wm + offs_n[None, :] * stride_wn)\n",
    "\n",
    "#     # -----------------------------------------------------------\n",
    "#     # Iterate to compute a block of the `z` vector.\n",
    "#     # We accumulate into a block of `BLOCK_SIZE_N` elements of FP32 values for higher accuracy.\n",
    "#     accumulator = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n",
    "#     for m in range(0, tl.cdiv(M, BLOCK_SIZE_M)):\n",
    "#         # Load the next block of `x` and `W`, generate a mask by checking along `M`.\n",
    "#         # If it is out of bounds, set it to 0.\n",
    "#         x = tl.load(x_ptrs, mask=offs_m < M - m * BLOCK_SIZE_M, other=0.0)[:, None]  # Force broadcast to correct shape\n",
    "#         w = tl.load(w_ptrs, mask=offs_m[:, None] < M - m * BLOCK_SIZE_M, other=0.0)\n",
    "\n",
    "#         # Since `w` is ternary, we only really care about the sign of the element in the array, and so\n",
    "#         # we just need to perform two conditional checks\n",
    "#         elements_to_sum = tl.where(w > 0, x, tl.where(w < 0, -x, tl.zeros_like(x)))\n",
    "#         accumulator = accumulator + tl.sum(elements_to_sum, axis=0)  # Sum along the `M` direction\n",
    "\n",
    "#         # Advance the ptrs to the next `M` block.\n",
    "#         x_ptrs += BLOCK_SIZE_M * stride_xm\n",
    "#         w_ptrs += BLOCK_SIZE_M * stride_wm\n",
    "\n",
    "#     z = accumulator / scale  # TODO: Do we want to reduce precision back to FP16?\n",
    "\n",
    "#     # -----------------------------------------------------------\n",
    "#     # Write back the block of the output vector `z` with masks.\n",
    "#     offs_z = pid_0 * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "#     z_ptrs = z_ptr + offs_z\n",
    "#     z_mask = offs_z < N\n",
    "#     tl.store(z_ptrs, z, mask=z_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xexLN2Dz0GM7"
   },
   "source": [
    "Create a convenience wrapper function that handles the checks and kernel calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-MqHjzD30GeM"
   },
   "outputs": [],
   "source": [
    "# # ruff: noqa: E731, S101, N806\n",
    "# def ternary_mul_2d(x, w, scale):\n",
    "#     # Check constraints\n",
    "#     assert len(x) == w.shape[0], \"Incompatible dimensions\"\n",
    "#     assert x.is_contiguous(), \"x must be contiguous\"\n",
    "#     assert x.is_cuda and w.is_cuda\n",
    "\n",
    "#     # Get dimensions\n",
    "#     M, N = w.shape\n",
    "\n",
    "#     # Allocate output\n",
    "#     z = torch.empty((N,), device=x.device, dtype=torch.float32)  # TODO: Change precision?\n",
    "\n",
    "#     # 1D launch kernel where each block gets its own program\n",
    "#     grid = lambda META: (triton.cdiv(N, META[\"BLOCK_SIZE_N\"]),)\n",
    "\n",
    "#     # fmt: off\n",
    "#     ternary_mul_2d_kernel[grid](\n",
    "#         x, w, z,\n",
    "#         scale,\n",
    "#         M, N,\n",
    "#         x.stride(0),\n",
    "#         w.stride(0), w.stride(1)\n",
    "#     )\n",
    "#     # fmt: on\n",
    "\n",
    "#     return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QC4g_Sb8ggMk"
   },
   "source": [
    "Test the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vX7hI8CV0mFX"
   },
   "outputs": [],
   "source": [
    "# X_LEN = 256  # x is the 1D vector\n",
    "# W_LEN = 256  # W is the quantized weights matrix\n",
    "# W_SIZE = (X_LEN, W_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_QyszNe03Sr",
    "outputId": "fdf97a5a-9589-4856-e2c5-d2765fa244c6"
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YvvBIRqtgk1r"
   },
   "outputs": [],
   "source": [
    "# x = torch.rand(X_LEN, device=\"cuda\", dtype=torch.float32)\n",
    "# w = torch.tensor([-1., 0., 1.], device=\"cuda\", dtype=torch.float32)[torch.randint(2, W_SIZE)]\n",
    "# scale = torch.rand(1, dtype=torch.float32).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "dpUvXBwD05_E"
   },
   "outputs": [],
   "source": [
    "# torch_output = torch.matmul(x, w) / scale\n",
    "# print(torch_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FyLkFEHg1EPX"
   },
   "outputs": [],
   "source": [
    "# triton_output = ternary_mul_2d(x, w, scale)\n",
    "# print(triton_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.allclose(triton_output, torch_output, atol=1e-3):\n",
    "#     print(\"✅ Triton and Torch match\")\n",
    "# else:\n",
    "#     raise ValueError(\"❌ Triton and Torch differ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark the Triton kernel against the standard Torch implementation of matmul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref_lib = \"cuBLAS\"\n",
    "\n",
    "# configs = [\n",
    "#     triton.testing.Benchmark(\n",
    "#         x_names=[\"M\", \"N\"],  # Argument names to use as an x-axis for the plot\n",
    "#         x_vals=[128 * i for i in range(1, 31)],  # Different possible values for `x_name`\n",
    "#         line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "#         # Possible values for `line_arg`\n",
    "#         line_vals=[ref_lib.lower(), \"triton\"],  # Label name for the lines\n",
    "#         line_names=[ref_lib, \"Triton\"],  # Line styles\n",
    "#         styles=[(\"green\", \"-\"), (\"blue\", \"-\")],\n",
    "#         ylabel=\"GFLOPS\",  # Label name for the y-axis\n",
    "#         plot_name=\"ternary-mul-2d-performance\",  # Name for the plot, used also as a file name for saving the plot.\n",
    "#         args={},\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "# @triton.testing.perf_report(configs)\n",
    "# def benchmark(M, N, provider):\n",
    "#     print(f\"Trial when M = {M} and N = {N} for {provider}\")\n",
    "#     x = torch.rand(M, device=\"cuda\")\n",
    "#     w = torch.tensor([-1.0, 0.0, 1.0], device=\"cuda\")[torch.randint(2, (M, N))]\n",
    "#     scale = torch.rand(1, dtype=torch.float32).item()\n",
    "\n",
    "#     quantiles = [0.5, 0.2, 0.8]\n",
    "#     if provider == ref_lib.lower():\n",
    "#         ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(x, w) / scale, quantiles=quantiles)\n",
    "#     if provider == \"triton\":\n",
    "#         ms, min_ms, max_ms = triton.testing.do_bench(lambda: ternary_mul_2d(x, w, scale), quantiles=quantiles)\n",
    "#     gflops = lambda ms: 2 * M * N * 1e-9 / (ms * 1e-3)\n",
    "#     return gflops(ms), gflops(max_ms), gflops(min_ms)\n",
    "\n",
    "\n",
    "# benchmark.run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D-2D Ternary Matrix Multipliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_autotune_config_2d2d():\n",
    "    return [\n",
    "        triton.Config(\n",
    "            {\"BLOCK_SIZE_M\": 2, \"BLOCK_SIZE_N\": 2, \"BLOCK_SIZE_K\": 2, \"GROUP_SIZE_M\": 2},\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return [\n",
    "        triton.Config(\n",
    "            {\"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 64, \"GROUP_SIZE_M\": 8}, num_stages=3, num_warps=8\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\"BLOCK_SIZE_M\": 64, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE_M\": 8}, num_stages=4, num_warps=4\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 128, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE_M\": 8}, num_stages=4, num_warps=4\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 64, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE_M\": 8}, num_stages=4, num_warps=4\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\"BLOCK_SIZE_M\": 64, \"BLOCK_SIZE_N\": 128, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE_M\": 8}, num_stages=4, num_warps=4\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 32, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE_M\": 8}, num_stages=4, num_warps=4\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\"BLOCK_SIZE_M\": 64, \"BLOCK_SIZE_N\": 32, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE_M\": 8}, num_stages=5, num_warps=2\n",
    "        ),\n",
    "        triton.Config(\n",
    "            {\"BLOCK_SIZE_M\": 32, \"BLOCK_SIZE_N\": 64, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE_M\": 8}, num_stages=5, num_warps=2\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    # return [\n",
    "    #     triton.Config(\n",
    "    #         {\"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 64, \"GROUP_SIZE_M\": 8}, num_stages=3, num_warps=8\n",
    "    #     ),\n",
    "    #     triton.Config(\n",
    "    #         {\"BLOCK_SIZE_M\": 64, \"BLOCK_SIZE_N\": 256, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE_M\": 8}, num_stages=4, num_warps=4\n",
    "    #     ),\n",
    "    #     triton.Config(\n",
    "    #         {\"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 128, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE_M\": 8}, num_stages=4, num_warps=4\n",
    "    #     ),\n",
    "    #     triton.Config(\n",
    "    #         {\"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 64, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE_M\": 8}, num_stages=4, num_warps=4\n",
    "    #     ),\n",
    "    #     triton.Config(\n",
    "    #         {\"BLOCK_SIZE_M\": 64, \"BLOCK_SIZE_N\": 128, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE_M\": 8}, num_stages=4, num_warps=4\n",
    "    #     ),\n",
    "    #     triton.Config(\n",
    "    #         {\"BLOCK_SIZE_M\": 128, \"BLOCK_SIZE_N\": 32, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE_M\": 8}, num_stages=4, num_warps=4\n",
    "    #     ),\n",
    "    #     triton.Config(\n",
    "    #         {\"BLOCK_SIZE_M\": 64, \"BLOCK_SIZE_N\": 32, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE_M\": 8}, num_stages=5, num_warps=2\n",
    "    #     ),\n",
    "    #     triton.Config(\n",
    "    #         {\"BLOCK_SIZE_M\": 32, \"BLOCK_SIZE_N\": 64, \"BLOCK_SIZE_K\": 32, \"GROUP_SIZE_M\": 8}, num_stages=5, num_warps=2\n",
    "    #     ),\n",
    "    # ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_autotune_config_2d2d():\n",
    "    if is_cuda():\n",
    "        return _get_autotune_config_2d2d()\n",
    "    else:\n",
    "        raise ValueError(\"Not on CUDA... can't use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudocode:\n",
    "```python\n",
    "# Do in parallel\n",
    "for m in range(0, M, BLOCK_SIZE_M):\n",
    "  # Do in parallel\n",
    "  for n in range(0, N, BLOCK_SIZE_N):\n",
    "    acc = zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=float32)\n",
    "    for k in range(0, K, BLOCK_SIZE_K):\n",
    "      a = A[m : m+BLOCK_SIZE_M, k : k+BLOCK_SIZE_K]\n",
    "      b = B[k : k+BLOCK_SIZE_K, n : n+BLOCK_SIZE_N]\n",
    "      acc += dot(a, b)\n",
    "    C[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N] = acc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_27397/3679571787.py:7: UserWarning: Compute capacity of CUDA device is below 7.0. The Triton compilation may fail terribly!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "@triton.autotune(\n",
    "    configs=get_autotune_config_2d2d(),\n",
    "    key=[\"M\", \"N\", \"K\"],\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_kernel(\n",
    "    # Pointers to matrices\n",
    "    a_ptr,\n",
    "    w_ptr,\n",
    "    z_ptr,\n",
    "    # Matrix dimensions\n",
    "    M,\n",
    "    N,\n",
    "    K,\n",
    "    # The stride variables represent how much to increase the ptr by when moving by 1\n",
    "    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
    "    # by to get the element one row down (A has M rows).\n",
    "    stride_am,\n",
    "    stride_ak,\n",
    "    stride_wk,\n",
    "    stride_wn,\n",
    "    stride_zm,\n",
    "    stride_zn,\n",
    "    # Meta-parameters\n",
    "    BLOCK_SIZE_M: tl.constexpr,\n",
    "    BLOCK_SIZE_N: tl.constexpr,\n",
    "    BLOCK_SIZE_K: tl.constexpr,\n",
    "    GROUP_SIZE_M: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Kernel for computing the matmul Z = A x W.\n",
    "    ``A`` has shape ``(M, K)``, ``W`` has shape ``(K, N)`` and ``Z`` has shape ``(M, N)``\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Map program ids `pid` to the block of C it should compute.\n",
    "    # This is done in a grouped ordering to promote L2 data reuse.\n",
    "    # See above `L2 Cache Optimizations` section for details.\n",
    "    pid = tl.program_id(axis=0)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + ((pid % num_pid_in_group) % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Create pointers for the first blocks of A and B.\n",
    "    # We will advance this pointer as we move in the K direction\n",
    "    # and accumulate\n",
    "    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n",
    "    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n",
    "    # See above `Pointer Arithmetic` section for details\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    # a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    a_ptrs = a_ptr + (pid_m * BLOCK_SIZE_M * stride_am + offs_k * stride_ak)\n",
    "    w_ptrs = w_ptr + (offs_k[:, None] * stride_wk + offs_bn[None, :] * stride_wn)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "        w = tl.load(w_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "        \n",
    "        # Inner 'dot' block\n",
    "        for m in range(0, BLOCK_SIZE_M):\n",
    "            a = tl.load(a_ptrs + m * stride_am + tl.arange(0, BLOCK_SIZE_K), mask=offs_k < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "            \n",
    "            # Since `w` is ternary, we only really care about the sign of the element in the array, and so\n",
    "            # we just need to perform two conditional checks\n",
    "            elements_to_sum = tl.where(w > 0, a, tl.where(w < 0, -a, tl.zeros_like(a)))\n",
    "            accumulator = accumulator + tl.sum(elements_to_sum, axis=0, keep_dims=True)  # Sum along the `M` direction\n",
    "        \n",
    "        # TODO: ADD\n",
    "\n",
    "        \n",
    "\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "        w_ptrs += BLOCK_SIZE_K * stride_wk\n",
    "\n",
    "    z = accumulator\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_zm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_zn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    z_ptrs = z_ptr + stride_zm * offs_zm[:, None] + stride_zn * offs_zn[None, :]\n",
    "    z_mask = (offs_zm[:, None] < M) & (offs_zn[None, :] < N)\n",
    "    tl.store(z_ptrs, z, mask=z_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul(a, b):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    assert a.is_contiguous(), \"Matrix A must be contiguous\"\n",
    "    M, K = a.shape\n",
    "    K, N = b.shape\n",
    "    # Allocates output.\n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(M, META[\"BLOCK_SIZE_M\"]) * triton.cdiv(N, META[\"BLOCK_SIZE_N\"]),)\n",
    "\n",
    "    # fmt: off\n",
    "    matmul_kernel[grid](\n",
    "        a, b, c,\n",
    "        M, N, K,\n",
    "        a.stride(0), a.stride(1),\n",
    "        b.stride(0), b.stride(1),\n",
    "        c.stride(0), c.stride(1)\n",
    "    )\n",
    "    # fmt: on\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QC4g_Sb8ggMk"
   },
   "source": [
    "Test the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "vX7hI8CV0mFX"
   },
   "outputs": [],
   "source": [
    "M = 2\n",
    "N = 3\n",
    "K = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_QyszNe03Sr",
    "outputId": "fdf97a5a-9589-4856-e2c5-d2765fa244c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6b723a9f50>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "YvvBIRqtgk1r"
   },
   "outputs": [],
   "source": [
    "a = torch.rand((M, K), device=\"cuda\", dtype=torch.float32)\n",
    "w = torch.tensor([-1., 0., 1.], device=\"cuda\", dtype=torch.float32)[torch.randint(2, (K, N))]\n",
    "# scale = torch.rand(1, dtype=torch.float32).item()\n",
    "scale = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "FyLkFEHg1EPX"
   },
   "outputs": [
    {
     "ename": "CompilationError",
     "evalue": "at 69:23:\n    # -----------------------------------------------------------\n    # Iterate to compute a block of the C matrix.\n    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n    # of fp32 values for higher accuracy.\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        # Load the next block of A and B, generate a mask by checking the K dimension.\n        w = tl.load(w_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n\n        # Inner 'dot' block\n        for m in range(0, BLOCK_SIZE_M):\n            print(\"m\", w[m, :])\n                       ^\nValueError('unsupported tensor index: int32[]')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCompilationError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m triton_output \u001b[38;5;241m=\u001b[39m \u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m scale\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(triton_output)\n",
      "Cell \u001b[0;32mIn[88], line 13\u001b[0m, in \u001b[0;36mmatmul\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     10\u001b[0m grid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m META: (triton\u001b[38;5;241m.\u001b[39mcdiv(M, META[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLOCK_SIZE_M\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m*\u001b[39m triton\u001b[38;5;241m.\u001b[39mcdiv(N, META[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBLOCK_SIZE_N\u001b[39m\u001b[38;5;124m\"\u001b[39m]),)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmatmul_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# fmt: on\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/keras-matmulless-b9IALFmu-py3.10/lib/python3.10/site-packages/triton/runtime/jit.py:326\u001b[0m, in \u001b[0;36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;124;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;124;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;124;03m    memorizes the grid.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/keras-matmulless-b9IALFmu-py3.10/lib/python3.10/site-packages/triton/runtime/autotuner.py:171\u001b[0m, in \u001b[0;36mAutotuner.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mpre_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     config\u001b[38;5;241m.\u001b[39mpre_hook({\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig\u001b[38;5;241m.\u001b[39mall_kwargs()})\n\u001b[0;32m--> 171\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/keras-matmulless-b9IALFmu-py3.10/lib/python3.10/site-packages/triton/runtime/jit.py:643\u001b[0m, in \u001b[0;36mJITFunction.run\u001b[0;34m(self, grid, warmup, *args, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# compile the kernel\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mASTSource(\u001b[38;5;28mself\u001b[39m, signature, constants, configs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 643\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m        \u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__dict__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache[device][key] \u001b[38;5;241m=\u001b[39m kernel\n\u001b[1;32m    650\u001b[0m \u001b[38;5;66;03m# Check that used global values have not changed.\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/keras-matmulless-b9IALFmu-py3.10/lib/python3.10/site-packages/triton/compiler/compiler.py:281\u001b[0m, in \u001b[0;36mcompile\u001b[0;34m(src, target, options)\u001b[0m\n\u001b[1;32m    279\u001b[0m codegen_fns \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mget_codegen_implementation()\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_ir\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    283\u001b[0m     filter_traceback(e)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/keras-matmulless-b9IALFmu-py3.10/lib/python3.10/site-packages/triton/compiler/compiler.py:113\u001b[0m, in \u001b[0;36mASTSource.make_ir\u001b[0;34m(self, options, codegen_fns, context)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_ir\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, codegen_fns, context):\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mast_to_ttir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodegen_fns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcodegen_fns\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mCompilationError\u001b[0m: at 69:23:\n    # -----------------------------------------------------------\n    # Iterate to compute a block of the C matrix.\n    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n    # of fp32 values for higher accuracy.\n    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        # Load the next block of A and B, generate a mask by checking the K dimension.\n        w = tl.load(w_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)\n\n        # Inner 'dot' block\n        for m in range(0, BLOCK_SIZE_M):\n            print(\"m\", w[m, :])\n                       ^\nValueError('unsupported tensor index: int32[]')"
     ]
    }
   ],
   "source": [
    "triton_output = matmul(a, w) / scale\n",
    "print(triton_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "dpUvXBwD05_E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000, -0.9860, -2.0640],\n",
      "        [ 0.0000, -0.2458, -1.3502]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "torch_output = torch.matmul(a, w) / scale\n",
    "print(torch_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "❌ Triton and Torch differ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Triton and Torch match\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m❌ Triton and Torch differ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: ❌ Triton and Torch differ"
     ]
    }
   ],
   "source": [
    "if torch.allclose(triton_output, torch_output, atol=1e-3):\n",
    "    print(\"✅ Triton and Torch match\")\n",
    "else:\n",
    "    raise ValueError(\"❌ Triton and Torch differ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_lib = \"cuBLAS\"\n",
    "\n",
    "configs = [\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=[\"M\", \"N\", \"K\"],  # Argument names to use as an x-axis for the plot\n",
    "        x_vals=[128 * i for i in range(2, 10)],  # Different possible values for `x_name`\n",
    "        line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "        # Possible values for `line_arg`\n",
    "        line_vals=[ref_lib.lower(), \"triton\"],  # Label name for the lines\n",
    "        line_names=[ref_lib, \"Triton\"],  # Line styles\n",
    "        styles=[(\"green\", \"-\"), (\"blue\", \"-\")],\n",
    "        ylabel=\"TFLOPS\",  # Label name for the y-axis\n",
    "        plot_name=\"matmul-performance\",  # Name for the plot, used also as a file name for saving the plot.\n",
    "        args = {}\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(M, N, K, provider):\n",
    "    a = torch.randn((M, K), device=\"cuda\", dtype=torch.float16)\n",
    "    b = torch.randn((K, N), device=\"cuda\", dtype=torch.float16)\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    if provider == ref_lib.lower():\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)\n",
    "    if provider == \"triton\":\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)\n",
    "    perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "\n",
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Ternary Multiplication Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autotune config should be similar to the 2D case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_autotune_config_3d():\n",
    "#     if is_cuda():\n",
    "#         return _get_autotune_config_2d()\n",
    "#     else:\n",
    "#         raise ValueError(\"Not on CUDA... can't use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel's rough pseudocode algorithm is as follows.\n",
    "```python\n",
    "k = 0  # Output vector pointer\n",
    "\n",
    "# Do in parallel\n",
    "for i in range(0, num_elem_per_matrix, matrix_stride):\n",
    "    # Do in parallel\n",
    "    for j in range(0, num_elem_per_vector, vector_stride):\n",
    "        matrix_elements = w_flat[i : i+matrix_stride].reshape(matrix_shape)\n",
    "        vector_elements = x_flat[j : j+vector_stride]\n",
    "        product = _2d_ternary_multiplication(vector_elements, matrix_elements, scale)\n",
    "        output_flat[k : k+output_stride] = product\n",
    "        k += output_stride\n",
    "\n",
    "return output_flat.reshape(output_shape)\n",
    "```\n",
    "\n",
    "Note that `matrix_stride` and `vector_stride` may not be power-of-two values. Thus we will employ padding to ensure that achieve this, but using a mask to ensure that we don't get memory access errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ruff: noqa: N803, PLR2044\n",
    "# @triton.autotune(\n",
    "#     configs=get_autotune_config_3d(),\n",
    "#     key=[\"M\", \"N\"],\n",
    "# )\n",
    "# @triton.jit\n",
    "# def ternary_mul_3d_kernel(\n",
    "#     # Pointers to arrays\n",
    "#     x_ptr,\n",
    "#     w_ptr,\n",
    "#     z_ptr,\n",
    "#     # Scaling factor\n",
    "#     scale,\n",
    "#     # `W` matrix dimensions\n",
    "#     K: tl.constexpr,\n",
    "#     M: tl.constexpr,\n",
    "#     N: tl.constexpr,\n",
    "#     # Strides\n",
    "#     stride_xk,\n",
    "#     stride_xm,\n",
    "#     stride_wk,\n",
    "#     stride_wm,\n",
    "#     stride_wn,\n",
    "#     # Meta-parameters\n",
    "#     BLOCK_SIZE_M: tl.constexpr,\n",
    "#     BLOCK_SIZE_N: tl.constexpr,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Kernel for computing the batched ternary multiplication\n",
    "#         z = xW\n",
    "#     `x` has shape `(K, M)`, `W` has shape `(K, M, N)`, and `z` has shape `(K, K, N)`.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # -----------------------------------------------------------\n",
    "#     # Map `pid` to the correct `x` vector and `W` matrix blocks that we are computing\n",
    "#     pid_0 = tl.program_id(axis=0)\n",
    "#     pid_1 = tl.program_id(axis=1)\n",
    "\n",
    "#     pid_x = pid_1 % K\n",
    "#     pid_w = pid_1 // K\n",
    "\n",
    "#     # ----------------------------------------------------------\n",
    "#     # Create pointers for the `x` vector and `W` matrix\n",
    "#     offs_m = tl.arange(0, BLOCK_SIZE_M)\n",
    "#     offs_n = (pid_0 * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N  # Guard against wrong offsets\n",
    "\n",
    "#     x_ptrs = x_ptr + pid_x * stride_xk + offs_m\n",
    "#     w_ptrs = w_ptr + pid_w * stride_wk + (offs_m[:, None] * stride_wm + offs_n[None, :] * stride_wn)\n",
    "\n",
    "#     # -----------------------------------------------------------\n",
    "#     # Iterate to compute a block of the `z` vector.\n",
    "#     # We accumulate into a block of `BLOCK_SIZE_N` elements of FP32 values for higher accuracy.\n",
    "#     accumulator = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n",
    "\n",
    "#     for m in range(0, tl.cdiv(M, BLOCK_SIZE_M)):\n",
    "#         # Load the next block of `x` and `W`, generate a mask by checking along `M`.\n",
    "#         # If it is out of bounds, set it to 0.\n",
    "#         x = tl.load(x_ptrs, mask=offs_m < M - m * BLOCK_SIZE_M, other=0.0)[:, None]  # Force broadcast to correct shape\n",
    "#         w = tl.load(w_ptrs, mask=offs_m[:, None] < M - m * BLOCK_SIZE_M, other=0.0)\n",
    "\n",
    "#         # Since `w` is ternary, we only really care about the sign of the element in the array, and so  we just need to\n",
    "#         # perform two conditional checks\n",
    "#         elements_to_sum = tl.where(w > 0, x, tl.where(w < 0, -x, tl.zeros_like(x)))\n",
    "#         accumulator = accumulator + tl.sum(elements_to_sum, axis=0)  # Sum along the `M` direction\n",
    "\n",
    "#         # Advance the ptrs to the next `M` block.\n",
    "#         x_ptrs += BLOCK_SIZE_M * stride_xm\n",
    "#         w_ptrs += BLOCK_SIZE_M * stride_wm\n",
    "\n",
    "#     z = accumulator / scale  # TODO: Do we want to reduce precision back to FP16?\n",
    "\n",
    "#     # -----------------------------------------------------------\n",
    "#     # Write back the block of the output vector `z` with masks\n",
    "#     offs_z = pid_0 * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "#     z_ptrs = z_ptr + pid_1 * N + offs_z\n",
    "#     z_mask = offs_z < N\n",
    "#     tl.store(z_ptrs, z, mask=z_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a convenience wrapper function that handles the checks and kernel calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ruff: noqa: E731, S101, N806\n",
    "# def ternary_mul_3d(x, w, scale):\n",
    "#     # Check constraints\n",
    "#     assert w.ndim == 3, \"Weight matrix does not have suitable dimensionality\"\n",
    "#     assert x.shape[-1] == w.shape[-2], \"Incompatible dimensions\"\n",
    "#     assert x.shape[0] == w.shape[0], \"Incompatible batch sizes\"\n",
    "#     assert x.is_contiguous(), \"x must be contiguous\"\n",
    "#     assert x.is_cuda and w.is_cuda\n",
    "\n",
    "#     # Get dimensions\n",
    "#     K, M, N = w.shape\n",
    "\n",
    "#     # Allocate output\n",
    "#     z = torch.zeros((K, K, N), device=x.device)\n",
    "\n",
    "#     # 2D launch kernel\n",
    "#     grid = lambda META: (triton.cdiv(N, META[\"BLOCK_SIZE_N\"]), K * K)\n",
    "\n",
    "#     # fmt: off\n",
    "#     ternary_mul_3d_kernel[grid](\n",
    "#         x, w, z,\n",
    "#         scale,\n",
    "#         K, M, N,\n",
    "#         x.stride(0), x.stride(1), w.stride(0), w.stride(1), w.stride(2)\n",
    "#     )\n",
    "#     # fmt: on\n",
    "\n",
    "#     return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_SHAPE = (3, 5)     # x is the vector\n",
    "# W_SHAPE = (3, 5, 7)  # W is the quantized weights matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvvBIRqtgk1r"
   },
   "outputs": [],
   "source": [
    "# x = torch.rand(X_SHAPE, device=\"cuda\", dtype=torch.float32)\n",
    "# w = torch.tensor([-1., 0., 1.], device=\"cuda\", dtype=torch.float32)[torch.randint(2, W_SHAPE)]\n",
    "# scale = torch.rand(1, dtype=torch.float32).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpUvXBwD05_E"
   },
   "outputs": [],
   "source": [
    "# torch_output = torch.matmul(x, w) / scale\n",
    "# print(torch_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FyLkFEHg1EPX"
   },
   "outputs": [],
   "source": [
    "# triton_output = ternary_mul_3d(x, w, scale)\n",
    "# print(triton_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.allclose(triton_output, torch_output, atol=1e-3):\n",
    "#     print(\"✅ Triton and Torch match\")\n",
    "# else:\n",
    "#     raise ValueError(\"❌ Triton and Torch differ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark the Triton kernel against the standard Torch implementation of matmul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref_lib = \"cuBLAS\"\n",
    "# K = 4\n",
    "\n",
    "# configs = [\n",
    "#     triton.testing.Benchmark(\n",
    "#         x_names=[\"M\", \"N\"],  # Argument names to use as an x-axis for the plot\n",
    "#         x_vals=[128 * i for i in range(11, 16)],  # Different possible values for `x_name`\n",
    "#         line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "#         # Possible values for `line_arg`\n",
    "#         line_vals=[ref_lib.lower(), \"triton\"],  # Label name for the lines\n",
    "#         line_names=[ref_lib, \"Triton\"],  # Line styles\n",
    "#         styles=[(\"green\", \"-\"), (\"blue\", \"-\")],\n",
    "#         ylabel=\"GFLOPS\",  # Label name for the y-axis\n",
    "#         plot_name=\"ternary-mul-3d-performance\",  # Name for the plot, used also as a file name for saving the plot.\n",
    "#         args={},\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "# @triton.testing.perf_report(configs)\n",
    "# def benchmark(M, N, provider):\n",
    "#     print(f\"Trial when K = {K}, M = {M}, N = {N}, for {provider}\")\n",
    "#     x = torch.rand((K, M), device=\"cuda\")\n",
    "#     w = torch.tensor([-1.0, 0.0, 1.0], device=\"cuda\")[torch.randint(2, (K, M, N))]\n",
    "#     scale = torch.rand(1, dtype=torch.float32).item()\n",
    "\n",
    "#     quantiles = [0.5, 0.2, 0.8]\n",
    "#     if provider == ref_lib.lower():\n",
    "#         ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(x, w) / scale, quantiles=quantiles)\n",
    "#     if provider == \"triton\":\n",
    "#         ms, min_ms, max_ms = triton.testing.do_bench(lambda: ternary_mul_3d(x, w, scale), quantiles=quantiles)\n",
    "#     gflops = lambda ms: 2 * K * M * N * 1e-9 / (ms * 1e-3)\n",
    "#     return gflops(ms), gflops(max_ms), gflops(min_ms)\n",
    "\n",
    "\n",
    "# benchmark.run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Ternary Multiplication Kernel (DOES NOT WORK FOR 4D AND ABOVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autotune config should be similar to the 2D case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_autotune_config():\n",
    "#     if is_cuda():\n",
    "#         return _get_autotune_config_2d()\n",
    "#     else:\n",
    "#         raise ValueError(\"Not on CUDA... can't use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel's rough pseudocode algorithm is as follows.\n",
    "```python\n",
    "k = 0  # Output vector pointer\n",
    "\n",
    "# Do in parallel\n",
    "for i in range(0, num_elem_per_matrix, matrix_stride):\n",
    "    # Do in parallel\n",
    "    for j in range(0, num_elem_per_vector, vector_stride):\n",
    "        matrix_elements = w_flat[i : i+matrix_stride].reshape(matrix_shape)\n",
    "        vector_elements = x_flat[j : j+vector_stride]\n",
    "        product = _2d_ternary_multiplication(vector_elements, matrix_elements, scale)\n",
    "        output_flat[k : k+output_stride] = product\n",
    "        k += output_stride\n",
    "\n",
    "return output_flat.reshape(output_shape)\n",
    "```\n",
    "\n",
    "Note that `matrix_stride` and `vector_stride` may not be power-of-two values. Thus we will employ padding to ensure that achieve this, but using a mask to ensure that we don't get memory access errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ruff: noqa: N803, PLR2044\n",
    "# @triton.autotune(\n",
    "#     configs=get_autotune_config(),\n",
    "#     key=[\"M\", \"N\"],\n",
    "# )\n",
    "# @triton.jit\n",
    "# def ternary_mul_kernel(\n",
    "#     # Pointers to arrays\n",
    "#     x_ptr,\n",
    "#     w_ptr,\n",
    "#     z_ptr,\n",
    "#     # Scaling factor\n",
    "#     scale,\n",
    "#     # # `W` matrix dimensions\n",
    "#     M: tl.constexpr,\n",
    "#     N: tl.constexpr,\n",
    "#     # Strides\n",
    "#     matrix_stride,\n",
    "#     vector_stride,\n",
    "#     stride_xm,\n",
    "#     stride_wm,\n",
    "#     stride_wn,\n",
    "#     # Counts of matrices and vectors\n",
    "#     num_stacked_matrices,\n",
    "#     num_stacked_vectors,\n",
    "#     # Meta-parameters\n",
    "#     BLOCK_SIZE_M: tl.constexpr,\n",
    "#     BLOCK_SIZE_N: tl.constexpr,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Kernel for computing the ternary multiplication\n",
    "#         z = xW\n",
    "#     `x` has shape `(..., M)`, `W` has shape `(..., M, N)`, and `z` has shape `(..., N)`.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # -----------------------------------------------------------\n",
    "#     # Map `pid` to the correct `x` vector and `W` matrix blocks that we are computing\n",
    "#     pid_0 = tl.program_id(axis=0)\n",
    "#     pid_1 = tl.program_id(axis=1)\n",
    "    \n",
    "#     pid_vector = pid_1 % num_stacked_vectors\n",
    "#     pid_matrix = pid_1 // num_stacked_vectors\n",
    "    \n",
    "#     # ----------------------------------------------------------\n",
    "#     # Create pointers for the `x` vector and `W` matrix\n",
    "#     offs_m = tl.arange(0, BLOCK_SIZE_M)\n",
    "#     offs_n = (pid_0 * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N  # Guard against wrong offsets\n",
    "\n",
    "#     offs_x = pid_vector * vector_stride + offs_m\n",
    "#     offs_w = pid_matrix * matrix_stride + (offs_m[:, None] * stride_wm + offs_n[None, :] * stride_wn)\n",
    "    \n",
    "#     x_ptrs = x_ptr + offs_x\n",
    "#     w_ptrs = w_ptr + offs_w\n",
    "    \n",
    "#     # -----------------------------------------------------------\n",
    "#     # Iterate to compute a block of the `z` vector.\n",
    "#     # We accumulate into a block of `BLOCK_SIZE_N` elements of FP32 values for higher accuracy.\n",
    "#     accumulator = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n",
    "\n",
    "#     for m in range(0, tl.cdiv(M, BLOCK_SIZE_M)):\n",
    "#         # Load the next block of `x` and `W`, generate a mask by checking along `M`.\n",
    "#         # If it is out of bounds, set it to 0.\n",
    "#         x = tl.load(x_ptrs, mask=offs_m < M - m * BLOCK_SIZE_M, other=0.0)[:, None]  # Force broadcast to correct shape\n",
    "#         w = tl.load(w_ptrs, mask=offs_m[:, None] < M - m * BLOCK_SIZE_M, other=0.0)\n",
    "\n",
    "#         # Since `w` is ternary, we only really care about the sign of the element in the array, and so  we just need to\n",
    "#         # perform two conditional checks\n",
    "#         elements_to_sum = tl.where(w > 0, x, tl.where(w < 0, -x, tl.zeros_like(x)))\n",
    "#         accumulator = accumulator + tl.sum(elements_to_sum, axis=0)  # Sum along the `M` direction\n",
    "\n",
    "#         # Advance the ptrs to the next `M` block.\n",
    "#         x_ptrs += BLOCK_SIZE_M * stride_xm\n",
    "#         w_ptrs += BLOCK_SIZE_M * stride_wm\n",
    "\n",
    "#     z = accumulator / scale  # TODO: Do we want to reduce precision back to FP16?\n",
    "\n",
    "#     # -----------------------------------------------------------\n",
    "#     # Write back the block of the output vector `z` with masks\n",
    "#     offs_z = pid_0 * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "#     z_ptrs = z_ptr + pid_1 * N + offs_z\n",
    "#     z_mask = offs_z < N\n",
    "#     tl.store(z_ptrs, z, mask=z_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a convenience wrapper function that handles the checks and kernel calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ruff: noqa: E731, S101, N806\n",
    "# def ternary_mul(x, w, scale):\n",
    "#     # Check constraints\n",
    "#     assert x.ndim == w.ndim - 1, \"Incompatible dimensionality\"\n",
    "#     assert x.shape[-1] == w.shape[-2], \"Incompatible dimensions\"\n",
    "#     assert x.is_contiguous(), \"x must be contiguous\"\n",
    "\n",
    "#     assert x.is_cuda and w.is_cuda\n",
    "\n",
    "#     # Otherwise, we need to treat what we have as a stack of matrices.\n",
    "#     # First we get the number of stacked matrices and vectors that we need to process\n",
    "#     w_shape = w.shape\n",
    "#     x_shape = x.shape\n",
    "\n",
    "#     num_stacked_matrices = 1\n",
    "#     num_stacked_vectors = 1\n",
    "#     for i in range(w.ndim - 2):  # The last 2 indices are the matrices\n",
    "#         num_stacked_matrices *= w_shape[i]\n",
    "#         num_stacked_vectors *= x_shape[i]\n",
    "\n",
    "#     # Identify the shape of the matrices and vectors that will actually be multiplied\n",
    "#     matrix_shape = (w_shape[-2], w_shape[-1])\n",
    "#     matrix_stride = w_shape[-2] * w_shape[-1]\n",
    "#     vector_stride = x_shape[-1]\n",
    "    \n",
    "#     # Get dimensions\n",
    "#     M, N = w_shape[-2], w_shape[-1]\n",
    "\n",
    "#     # Determine output size and shape\n",
    "#     if w.ndim == 2:\n",
    "#         output_shape = [w_shape[-1]]\n",
    "#         num_processes = 1  # No need for 2D launch grid\n",
    "#     else:\n",
    "#         output_shape = [*list(w_shape[:-2]), x_shape[-2], w_shape[-1]]\n",
    "#         num_processes = num_stacked_matrices * x_shape[-2]\n",
    "#     output = torch.zeros(output_shape, device=x.device)\n",
    "    \n",
    "#     # 2D launch kernel\n",
    "#     grid = lambda META: (triton.cdiv(N, META[\"BLOCK_SIZE_N\"]), num_processes)\n",
    "    \n",
    "#     # fmt: off\n",
    "#     ternary_mul_kernel[grid](\n",
    "#         x, w, output,\n",
    "#         scale,\n",
    "#         M, N,\n",
    "#         matrix_stride, vector_stride, x.stride(-1), w.stride(-2), w.stride(-1),\n",
    "#         num_stacked_matrices, num_stacked_vectors\n",
    "#     )\n",
    "#     # fmt: on\n",
    "\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QC4g_Sb8ggMk"
   },
   "source": [
    "Test the kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vX7hI8CV0mFX"
   },
   "outputs": [],
   "source": [
    "# # X_SHAPE = (   3, 3, 1)  # x is the vector\n",
    "# # W_SHAPE = (3, 3, 1, 2)  # W is the quantized weights matrix\n",
    "# X_SHAPE = (3, 1)  # x is the vector\n",
    "# W_SHAPE = (3, 1, 2)  # W is the quantized weights matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YvvBIRqtgk1r"
   },
   "outputs": [],
   "source": [
    "# x = torch.rand(X_SHAPE, device=\"cuda\", dtype=torch.float32)\n",
    "# w = torch.tensor([-1., 0., 1.], device=\"cuda\", dtype=torch.float32)[torch.randint(2, W_SHAPE)]\n",
    "# scale = torch.rand(1, dtype=torch.float32).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpUvXBwD05_E"
   },
   "outputs": [],
   "source": [
    "# torch_output = torch.matmul(x, w) / scale\n",
    "# print(torch_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FyLkFEHg1EPX"
   },
   "outputs": [],
   "source": [
    "# triton_output = ternary_mul(x, w, scale)\n",
    "# print(triton_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.allclose(triton_output, torch_output, atol=1e-3):\n",
    "#     print(\"✅ Triton and Torch match\")\n",
    "# else:\n",
    "#     raise ValueError(\"❌ Triton and Torch differ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref_lib = \"cuBLAS\"\n",
    "# K = 8\n",
    "\n",
    "# configs = [\n",
    "#     triton.testing.Benchmark(\n",
    "#         x_names=[\"M\", \"N\"],  # Argument names to use as an x-axis for the plot\n",
    "#         x_vals=[128 * i for i in range(1, 11)],  # Different possible values for `x_name`\n",
    "#         line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "#         # Possible values for `line_arg`\n",
    "#         line_vals=[ref_lib.lower(), \"triton\"],  # Label name for the lines\n",
    "#         line_names=[ref_lib, \"Triton\"],  # Line styles\n",
    "#         styles=[(\"green\", \"-\"), (\"blue\", \"-\")],\n",
    "#         ylabel=\"GFLOPS\",  # Label name for the y-axis\n",
    "#         plot_name=\"ternary-mul-general-performance\",  # Name for the plot, used also as a file name for saving the plot.\n",
    "#         args={},\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "# @triton.testing.perf_report(configs)\n",
    "# def benchmark(M, N, provider):\n",
    "#     print(f\"Trial when K = {K}, M = {M}, N = {N}, for {provider}\")\n",
    "#     x = torch.rand((K, M), device=\"cuda\")\n",
    "#     w = torch.tensor([-1.0, 0.0, 1.0], device=\"cuda\")[torch.randint(2, (K, M, N))]\n",
    "#     scale = torch.rand(1, dtype=torch.float32).item()\n",
    "\n",
    "#     quantiles = [0.5, 0.2, 0.8]\n",
    "#     if provider == ref_lib.lower():\n",
    "#         ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(x, w) / scale, quantiles=quantiles)\n",
    "#     if provider == \"triton\":\n",
    "#         ms, min_ms, max_ms = triton.testing.do_bench(lambda: ternary_mul(x, w, scale), quantiles=quantiles)\n",
    "#     gflops = lambda ms: 2 * K * M * N * 1e-9 / (ms * 1e-3)\n",
    "#     return gflops(ms), gflops(max_ms), gflops(min_ms)\n",
    "\n",
    "\n",
    "# benchmark.run(show_plots=True, print_data=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
