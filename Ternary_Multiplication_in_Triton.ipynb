{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PdrF5MIoMXX8"
   },
   "source": [
    "# Ternary Multiplication in Triton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGNuCpfuMpAU"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DAdIYpUMjyD"
   },
   "source": [
    "Check the installed triton version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hNPo7UMLMlVY"
   },
   "outputs": [],
   "source": [
    "import triton\n",
    "assert triton.__version__ == \"3.0.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zh04HhvLMznK"
   },
   "source": [
    "Import other needed stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uglYrA26M0vg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton.language as tl\n",
    "from jaxtyping import Float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pI2dDdjsbPnB"
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "mzcLVuDLM30m"
   },
   "outputs": [],
   "source": [
    "def get_current_target():\n",
    "    return triton.runtime.driver.active.get_current_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "57uA2WypcCAR"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "def is_cuda():\n",
    "    current_target = get_current_target()\n",
    "    if current_target.backend != \"cuda\":\n",
    "        return False\n",
    "\n",
    "    if current_target.arch < 70:  # CUDA compute capacity is below 7.0, which is minimum 'stable' supported\n",
    "        warnings.warn(\"Compute capcity of CUDA device is below 7.0. The Triton compilation may fail terribly!\")\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Xs_RIxorbvP3"
   },
   "outputs": [],
   "source": [
    "# def is_hip_mi200():\n",
    "#     target = triton.runtime.driver.active.get_current_target()\n",
    "#     return target.backend == \"hip\" and target.arch == \"gfx90a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ex9yAjaFcFwd"
   },
   "outputs": [],
   "source": [
    "# is_hip_mi200()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "flaq-3Gez1dl"
   },
   "outputs": [],
   "source": [
    "def get_cuda_autotune_config():\n",
    "    # return [\n",
    "    #     triton.Config(\n",
    "    #         {\n",
    "    #             \"BLOCK_SIZE_M\": 128,\n",
    "    #             \"BLOCK_SIZE_N\": 256,\n",
    "    #         },\n",
    "    #         num_stages=3,\n",
    "    #         num_warps=8,\n",
    "    #     ),\n",
    "    #     triton.Config(\n",
    "    #         {\n",
    "    #             \"BLOCK_SIZE_M\": 64,\n",
    "    #             \"BLOCK_SIZE_N\": 256,\n",
    "    #         },\n",
    "    #         num_stages=4,\n",
    "    #         num_warps=4,\n",
    "    #     ),\n",
    "    #     triton.Config(\n",
    "    #         {\n",
    "    #             \"BLOCK_SIZE_M\": 128,\n",
    "    #             \"BLOCK_SIZE_N\": 128,\n",
    "    #         },\n",
    "    #         num_stages=4,\n",
    "    #         num_warps=4,\n",
    "    #     ),\n",
    "    #     triton.Config(\n",
    "    #         {\n",
    "    #             \"BLOCK_SIZE_M\": 128,\n",
    "    #             \"BLOCK_SIZE_N\": 64,\n",
    "    #         },\n",
    "    #         num_stages=4,\n",
    "    #         num_warps=4,\n",
    "    #     ),\n",
    "    #     triton.Config(\n",
    "    #         {\n",
    "    #             \"BLOCK_SIZE_M\": 64,\n",
    "    #             \"BLOCK_SIZE_N\": 128,\n",
    "    #         },\n",
    "    #         num_stages=4,\n",
    "    #         num_warps=4,\n",
    "    #     ),\n",
    "    #     triton.Config(\n",
    "    #         {\n",
    "    #             \"BLOCK_SIZE_M\": 128,\n",
    "    #             \"BLOCK_SIZE_N\": 32,\n",
    "    #         },\n",
    "    #         num_stages=4,\n",
    "    #         num_warps=4,\n",
    "    #     ),\n",
    "    #     triton.Config(\n",
    "    #         {\n",
    "    #             \"BLOCK_SIZE_M\": 64,\n",
    "    #             \"BLOCK_SIZE_N\": 32,\n",
    "    #         },\n",
    "    #         num_stages=5,\n",
    "    #         num_warps=2,\n",
    "    #     ),\n",
    "    #     triton.Config(\n",
    "    #         {\n",
    "    #             \"BLOCK_SIZE_M\": 32,\n",
    "    #             \"BLOCK_SIZE_N\": 64,\n",
    "    #         },\n",
    "    #         num_stages=5,\n",
    "    #         num_warps=2,\n",
    "    #     ),\n",
    "    #     # Good config for fp8 inputs.\n",
    "    #     triton.Config(\n",
    "    #         {\n",
    "    #             \"BLOCK_SIZE_M\": 128,\n",
    "    #             \"BLOCK_SIZE_N\": 256,\n",
    "    #         },\n",
    "    #         num_stages=3,\n",
    "    #         num_warps=8,\n",
    "    #     ),\n",
    "    #     triton.Config(\n",
    "    #         {\n",
    "    #             \"BLOCK_SIZE_M\": 256,\n",
    "    #             \"BLOCK_SIZE_N\": 128,\n",
    "    #         },\n",
    "    #         num_stages=3,\n",
    "    #         num_warps=8,\n",
    "    #     ),\n",
    "    #     triton.Config(\n",
    "    #         {\n",
    "    #             \"BLOCK_SIZE_M\": 256,\n",
    "    #             \"BLOCK_SIZE_N\": 64,\n",
    "    #         },\n",
    "    #         num_stages=4,\n",
    "    #         num_warps=4,\n",
    "    #     ),\n",
    "    #     triton.Config(\n",
    "    #         {\n",
    "    #             \"BLOCK_SIZE_M\": 64,\n",
    "    #             \"BLOCK_SIZE_N\": 256,\n",
    "    #         },\n",
    "    #         num_stages=4,\n",
    "    #         num_warps=4,\n",
    "    #     ),\n",
    "    #     triton.Config(\n",
    "    #         {\n",
    "    #             \"BLOCK_SIZE_M\": 128,\n",
    "    #             \"BLOCK_SIZE_N\": 128,\n",
    "    #         },\n",
    "    #         num_stages=4,\n",
    "    #         num_warps=4,\n",
    "    #     ),\n",
    "    #     triton.Config(\n",
    "    #         {\n",
    "    #             \"BLOCK_SIZE_M\": 128,\n",
    "    #             \"BLOCK_SIZE_N\": 64,\n",
    "    #         },\n",
    "    #         num_stages=4,\n",
    "    #         num_warps=4,\n",
    "    #     ),\n",
    "    #     triton.Config(\n",
    "    #         {\n",
    "    #             \"BLOCK_SIZE_M\": 64,\n",
    "    #             \"BLOCK_SIZE_N\": 128,\n",
    "    #         },\n",
    "    #         num_stages=4,\n",
    "    #         num_warps=4,\n",
    "    #     ),\n",
    "    #     triton.Config(\n",
    "    #         {\n",
    "    #             \"BLOCK_SIZE_M\": 128,\n",
    "    #             \"BLOCK_SIZE_N\": 32,\n",
    "    #         },\n",
    "    #         num_stages=4,\n",
    "    #         num_warps=4,\n",
    "    #     ),\n",
    "    # ]\n",
    "    return [triton.Config({\"BLOCK_SIZE_M\": 2, \"BLOCK_SIZE_N\": 2})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gkI9jzg80Fkt"
   },
   "outputs": [],
   "source": [
    "def get_autotune_config():\n",
    "    if is_cuda():\n",
    "        return get_cuda_autotune_config()\n",
    "    else:\n",
    "        raise ValueError(\"Not on CUDA... can't use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main ternary multiplication kernel.\n",
    "\n",
    "The rough pseudocode algorithm is as follows.\n",
    "```python\n",
    "# Do in parallel\n",
    "for n in range(0, N, BLOCK_SIZE_N):\n",
    "    acc = zeros((BLOCK_SIZE_N,), dtype=float32)\n",
    "    for m in range(0, M, BLOCK_SIZE_M):\n",
    "        x_block = x[m : m+BLOCK_SIZE_M]\n",
    "        w_block = w[m : m+BLOCK_SIZE_M, n : n+BLOCK_SIZE_N]\n",
    "        \n",
    "        # Since `w` is ternary, we only really care about the sign of the element in the array, and so\n",
    "        # we just need to perform two conditional checks\n",
    "        elems_to_sum = tl.where(w_block > 0, x_block, tl.where(w_block < 0, -x_block, tl.zeros_like(x_block)))\n",
    "        acc += tl.sum(elems_to_sum)  # Sum along the M direction\n",
    "\n",
    "    acc = acc / scale\n",
    "    z[n : n+BLOCK_SIZE_N] = acc\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25316/1541291054.py:10: UserWarning: Compute capcity of CUDA device is below 7.0. The Triton compilation may fail terribly!\n",
      "  warnings.warn(\"Compute capcity of CUDA device is below 7.0. The Triton compilation may fail terribly!\")\n"
     ]
    }
   ],
   "source": [
    "# ruff: noqa: N803, PLR2044\n",
    "@triton.autotune(\n",
    "    configs=get_autotune_config(),\n",
    "    key=[\"M\", \"N\"],\n",
    ")\n",
    "@triton.jit\n",
    "def ternary_mul_kernel(\n",
    "    # Pointers to matrices\n",
    "    x_ptr,\n",
    "    w_ptr,\n",
    "    z_ptr,\n",
    "    # Scaling factor\n",
    "    scale,\n",
    "    # `W` matrix dimensions\n",
    "    M,\n",
    "    N,\n",
    "    # The stride variables represent how much to increase the ptr by when moving by 1\n",
    "    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
    "    # by to get the element one row down (A has M rows).\n",
    "    stride_xm,\n",
    "    stride_wm,\n",
    "    stride_wn,\n",
    "    # Meta-parameters\n",
    "    BLOCK_SIZE_M: tl.constexpr,\n",
    "    BLOCK_SIZE_N: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Kernel for computing the ternary multiplication\n",
    "        z = xW\n",
    "    `x` has shape `(1, M)`, `W` has shape `(M, N)`, and `z` has shape `(1, N)`.\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Map `pid` to the block of `z` that it should compute.\n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Create pointers for the first blocks of `x` and `W`.\n",
    "    # We will advance this pointer as we move in the `M` direction and accumulate.\n",
    "    # - `x_ptrs` is a block of `BLOCK_SIZE_M` pointers\n",
    "    # - `w_ptrs` is a block of pointers with shape `(BLOCK_SIZE_M, BLOCK_SIZE_N)`\n",
    "    offs_m = tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_n = (pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N  # Guard against wrong offsets\n",
    "    x_ptrs = x_ptr + offs_m\n",
    "    w_ptrs = w_ptr + (offs_m[:, None] * stride_wm + offs_n[None, :] * stride_wn)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the `z` vector.\n",
    "    # We accumulate into a block of `BLOCK_SIZE_N` elements of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_N,), dtype=tl.float32)\n",
    "    for m in range(0, tl.cdiv(M, BLOCK_SIZE_M)):\n",
    "        # Load the next block of `x` and `W`, generate a mask by checking the ??? dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        # TODO: Check masks\n",
    "        x = tl.load(x_ptrs, mask=offs_m < M - m * BLOCK_SIZE_M, other=0.0)[:, None]  # Force broadcast to correct shape here\n",
    "        w = tl.load(w_ptrs, mask=offs_m[:, None] < M - m * BLOCK_SIZE_M, other=0.0)\n",
    "\n",
    "        # Since `w` is ternary, we only really care about the sign of the element in the array, and so\n",
    "        # we just need to perform two conditional checks\n",
    "        elements_to_sum = tl.where(w > 0, x, tl.where(w < 0, -x, tl.zeros_like(x)))\n",
    "        accumulator = accumulator + tl.sum(elements_to_sum, axis=0)  # Sum along the `M` direction\n",
    "\n",
    "        # Advance the ptrs to the next `M` block.\n",
    "        x_ptrs += BLOCK_SIZE_M * stride_xm\n",
    "        w_ptrs += BLOCK_SIZE_M * stride_wm\n",
    "\n",
    "    accumulator = accumulator / scale\n",
    "    z = accumulator.to(tl.float16)\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output vector `z` with masks.\n",
    "    offs_z = pid * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    z_ptrs = z_ptr + offs_z\n",
    "    z_mask = offs_z < N\n",
    "    tl.store(z_ptrs, z, mask=z_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xexLN2Dz0GM7"
   },
   "source": [
    "We can now create a convenience wrapper function that only takes two input tensors, and (1) checks any shape constraint; (2) allocates the output; (3) launches the above kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "-MqHjzD30GeM"
   },
   "outputs": [],
   "source": [
    "# ruff: noqa: E731\n",
    "def ternary_mul(x, w, scale):\n",
    "    # Check constraints.\n",
    "    assert len(x) == w.shape[0], \"Incompatible dimensions\"\n",
    "    assert x.is_contiguous(), \"x must be contiguous\"\n",
    "\n",
    "    assert x.is_cuda and w.is_cuda\n",
    "\n",
    "    # Get dimensions\n",
    "    M, N = w.shape\n",
    "\n",
    "    # Allocate output\n",
    "    z = torch.empty((N,), device=x.device, dtype=torch.float16)  # TODO: Change precision?\n",
    "\n",
    "    # 1D launch kernel where each block gets its own program.\n",
    "    grid = lambda META: (triton.cdiv(N, META[\"BLOCK_SIZE_N\"]),)\n",
    "    ternary_mul_kernel[grid](\n",
    "        x, w, z,  #\n",
    "        scale,  #\n",
    "        M, N,  #\n",
    "        x.stride(0),  #\n",
    "        w.stride(0), w.stride(1)\n",
    "    )\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QC4g_Sb8ggMk"
   },
   "source": [
    "TESTING CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "vX7hI8CV0mFX"
   },
   "outputs": [],
   "source": [
    "X_LEN = 8  # x is the 1D vector\n",
    "W_LEN = 8  # W is the quantized weights matrix\n",
    "W_SIZE = (X_LEN, W_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q_QyszNe03Sr",
    "outputId": "fdf97a5a-9589-4856-e2c5-d2765fa244c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3c2968fdb0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "YvvBIRqtgk1r"
   },
   "outputs": [],
   "source": [
    "# x = torch.rand(X_LEN, device=\"cuda\")\n",
    "# w = torch.tensor([-1., 0., 1.], device=\"cuda\")[torch.randint(2, W_SIZE)]\n",
    "s = torch.tensor([1., 2, 4, 8], device=\"cuda\")\n",
    "w = torch.tensor([\n",
    "    [ 1.,  0.,  0.,  0.],\n",
    "    [ 0.,  1.,  1.,  0.],\n",
    "    [ 0., -1.,  0.,  1.],\n",
    "    [ 0.,  0.,  1., -1.]\n",
    "], device=\"cuda\")\n",
    "scale = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "dpUvXBwD05_E"
   },
   "outputs": [],
   "source": [
    "torch_output = torch.matmul(s, w) / scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "moDlAjlC2VoZ",
    "outputId": "05b74885-07a2-4b9c-ed17-28816955cbbe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1., -2., 10., -4.], device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "FyLkFEHg1EPX"
   },
   "outputs": [],
   "source": [
    "triton_output = ternary_mul(s, w, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5tEhCfZ9bvz",
    "outputId": "d847a553-0f21-4f76-deb5-53653b3bd281"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1., -2., 10., -4.], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1., -6.,  5.,  0.], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triton_output"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
